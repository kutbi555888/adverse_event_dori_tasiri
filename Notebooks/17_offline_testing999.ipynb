{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 17) Offline Testing — Best Model inference\n",
        "Maqsad:\n",
        "- `Models/best_model/optuna_logreg_best/optuna_logreg_best.joblib`\n",
        "- `..._thresholds.json`\n",
        "- `Data/Feature_Selected/<VERSION>/X_test.npz` (yoki Engineered_data)\n",
        "lar bilan **offline** predict + (agar bo‘lsa) testda tezkor baholash.\n",
        "\n",
        "**Eslatma:** Bu notebook `pandas .style` ishlatmaydi (jinja2 kerak emas). Hammasi `tabulate`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import joblib\n",
        "from tabulate import tabulate\n",
        "import zipfile\n",
        "\n",
        "# =========================\n",
        "# CONFIG (kerak bo‘lsa o‘zgartiring)\n",
        "# =========================\n",
        "VERSION = \"fe_v1_fs_chi2_v1\"          # Data/Feature_Selected yoki Data/Engineered_data ichidagi versiya\n",
        "PREFER_FEATURE_SELECTED = True        # True -> Feature_Selected, bo‘lmasa Engineered_data\n",
        "\n",
        "MODEL_NAME = \"optuna_logreg_best\"     # Models/best_model/<MODEL_NAME> folder\n",
        "TEXT_COL_FALLBACK = \"REAC_pt_symptom\" # agar raw csv dan text ko‘rsatmoqchi bo‘lsangiz\n",
        "\n",
        "N_SHOW = 10                          # random offline sample count\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def find_project_root(start: Path | None = None) -> Path:\n",
        "    start = start or Path.cwd()\n",
        "    # Data/Raw_data yoki Data/Processed bo‘lsa project root deb olamiz\n",
        "    for p in [start] + list(start.parents):\n",
        "        data = p / \"Data\"\n",
        "        raw = data / \"Raw_data\"\n",
        "        processed = data / \"Processed\"\n",
        "        models = p / \"Models\"\n",
        "        if (raw.exists() and any(raw.iterdir())) or (processed.exists() and any(processed.iterdir())) or models.exists():\n",
        "            return p\n",
        "    return start\n",
        "\n",
        "def find_data_dir(project_root: Path, version: str, prefer_fs: bool = True) -> tuple[Path, str]:\n",
        "    fs = project_root / \"Data\" / \"Feature_Selected\" / version\n",
        "    eng = project_root / \"Data\" / \"Engineered_data\" / version\n",
        "    if prefer_fs and (fs / \"X_test.npz\").exists(): return fs, \"Feature_Selected\"\n",
        "    if (eng / \"X_test.npz\").exists(): return eng, \"Engineered_data\"\n",
        "    if (fs / \"X_test.npz\").exists(): return fs, \"Feature_Selected\"\n",
        "    raise FileNotFoundError(f\"X_test.npz topilmadi: {fs} yoki {eng}\")\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "DATA_DIR, DATA_SOURCE = find_data_dir(PROJECT_ROOT, VERSION, PREFER_FEATURE_SELECTED)\n",
        "\n",
        "MODEL_DIR = PROJECT_ROOT / \"Models\" / \"best_model\" / MODEL_NAME\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT.resolve())\n",
        "print(\"DATA_SOURCE :\", DATA_SOURCE)\n",
        "print(\"DATA_DIR    :\", DATA_DIR.resolve())\n",
        "print(\"MODEL_DIR   :\", MODEL_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Load model + thresholds\n",
        "# =========================\n",
        "MODEL_PATH = MODEL_DIR / f\"{MODEL_NAME}.joblib\"\n",
        "THR_PATH   = MODEL_DIR / f\"{MODEL_NAME}_thresholds.json\"\n",
        "\n",
        "if not MODEL_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Model topilmadi: {MODEL_PATH}\")\n",
        "if not THR_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Thresholds topilmadi: {THR_PATH}\")\n",
        "\n",
        "model = joblib.load(MODEL_PATH)\n",
        "\n",
        "with open(THR_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    thr_dict = json.load(f)\n",
        "\n",
        "print(\"Loaded model:\", type(model))\n",
        "print(\"Threshold keys:\", len(thr_dict))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 2) Load X/Y + ids + meta\n",
        "# =========================\n",
        "META_PATH = DATA_DIR / \"engineered_meta.json\"\n",
        "if not META_PATH.exists():\n",
        "    raise FileNotFoundError(f\"engineered_meta.json topilmadi: {META_PATH}\")\n",
        "\n",
        "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "y_cols = meta[\"y_cols\"]  # masalan: [\"y_gastrointestinal\", ...]\n",
        "label_names = [c.replace(\"y_\", \"\", 1) for c in y_cols]\n",
        "\n",
        "# Threshold vector (y_cols tartibida)\n",
        "thr = np.array([float(thr_dict.get(name, 0.5)) for name in label_names], dtype=np.float32)\n",
        "\n",
        "X_test = sparse.load_npz(DATA_DIR / \"X_test.npz\").tocsr()\n",
        "Y_test_path = DATA_DIR / \"Y_test.npy\"\n",
        "Y_test = np.load(Y_test_path) if Y_test_path.exists() else None\n",
        "\n",
        "ids_test_path = DATA_DIR / \"ids_test.csv\"\n",
        "ids_test = pd.read_csv(ids_test_path) if ids_test_path.exists() else None\n",
        "\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"Y_test:\", None if Y_test is None else Y_test.shape)\n",
        "print(\"ids_test:\", None if ids_test is None else ids_test.shape)\n",
        "print(\"labels:\", len(label_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) Helpers: scoring, thresholds, metrics\n",
        "# =========================\n",
        "def prf_from_counts(tp: int, fp: int, fn: int):\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
        "    return prec, rec, f1\n",
        "\n",
        "def multilabel_micro_macro(Y_true: np.ndarray, Y_pred: np.ndarray) -> dict:\n",
        "    tp = int(((Y_true == 1) & (Y_pred == 1)).sum())\n",
        "    fp = int(((Y_true == 0) & (Y_pred == 1)).sum())\n",
        "    fn = int(((Y_true == 1) & (Y_pred == 0)).sum())\n",
        "    _, _, micro_f1 = prf_from_counts(tp, fp, fn)\n",
        "\n",
        "    f1s = []\n",
        "    for j in range(Y_true.shape[1]):\n",
        "        y = Y_true[:, j]\n",
        "        p = Y_pred[:, j]\n",
        "        tpj = int(((y == 1) & (p == 1)).sum())\n",
        "        fpj = int(((y == 0) & (p == 1)).sum())\n",
        "        fnj = int(((y == 1) & (p == 0)).sum())\n",
        "        _, _, f1j = prf_from_counts(tpj, fpj, fnj)\n",
        "        f1s.append(f1j)\n",
        "\n",
        "    return {\"micro_f1\": float(micro_f1), \"macro_f1\": float(np.mean(f1s))}\n",
        "\n",
        "def score_matrix(model, X):\n",
        "    # OVR(LogReg) -> predict_proba bo‘ladi\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        s = model.predict_proba(X)\n",
        "        # sklearn OVR proba: list of arrays OR array; unify\n",
        "        s = np.asarray(s)\n",
        "        return s, \"proba\"\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        return np.asarray(model.decision_function(X)), \"score\"\n",
        "    return np.asarray(model.predict(X)), \"binary\"\n",
        "\n",
        "def apply_thresholds(scores: np.ndarray, thr: np.ndarray) -> np.ndarray:\n",
        "    return (scores >= thr.reshape(1, -1)).astype(np.int8)\n",
        "\n",
        "def per_label_table(Y_true: np.ndarray, Y_pred: np.ndarray, label_names: list[str]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for j, name in enumerate(label_names):\n",
        "        y = Y_true[:, j]\n",
        "        p = Y_pred[:, j]\n",
        "        tp = int(((y == 1) & (p == 1)).sum())\n",
        "        fp = int(((y == 0) & (p == 1)).sum())\n",
        "        fn = int(((y == 1) & (p == 0)).sum())\n",
        "        tn = int(((y == 0) & (p == 0)).sum())\n",
        "        prec, rec, f1 = prf_from_counts(tp, fp, fn)\n",
        "        rows.append({\n",
        "            \"label\": name,\n",
        "            \"support\": int(y.sum()),\n",
        "            \"precision\": prec,\n",
        "            \"recall\": rec,\n",
        "            \"f1\": f1,\n",
        "            \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn\n",
        "        })\n",
        "    df = pd.DataFrame(rows).sort_values(\"f1\", ascending=False).reset_index(drop=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4) Offline TEST evaluation (agar Y_test bor bo‘lsa)\n",
        "# =========================\n",
        "scores_test, score_type = score_matrix(model, X_test)\n",
        "\n",
        "# score_type == \"binary\" bo‘lsa thresholds kerak emas, lekin biz baribir apply qilamiz:\n",
        "Y_pred_test = apply_thresholds(scores_test, thr) if score_type in (\"proba\",\"score\") else scores_test.astype(np.int8)\n",
        "\n",
        "print(\"score_type:\", score_type)\n",
        "print(\"scores_test:\", scores_test.shape)\n",
        "\n",
        "if Y_test is not None:\n",
        "    summary = multilabel_micro_macro(Y_test, Y_pred_test)\n",
        "    print(\"TEST summary:\", summary)\n",
        "\n",
        "    per_label = per_label_table(Y_test, Y_pred_test, label_names)\n",
        "    print(\"\\nTop 15 labels by F1 (TEST):\")\n",
        "    print(tabulate(per_label.head(15), headers=\"keys\", tablefmt=\"github\", showindex=False, floatfmt=\".6f\"))\n",
        "else:\n",
        "    print(\"Y_test topilmadi -> faqat prediction ko‘rsatamiz.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 5) Offline preview: random sample (ids_test bo‘lsa primaryid bilan)\n",
        "# =========================\n",
        "rng = np.random.default_rng(RANDOM_STATE)\n",
        "n = X_test.shape[0]\n",
        "idx = rng.choice(n, size=min(N_SHOW, n), replace=False)\n",
        "\n",
        "def labels_from_row(scores_row: np.ndarray, thr: np.ndarray, names: list[str]) -> list[str]:\n",
        "    return [names[j] for j in range(len(names)) if scores_row[j] >= thr[j]]\n",
        "\n",
        "def topk(scores_row: np.ndarray, names: list[str], k: int = 8):\n",
        "    j = np.argsort(scores_row)[::-1][:k]\n",
        "    return [(names[int(i)], float(scores_row[int(i)])) for i in j]\n",
        "\n",
        "rows = []\n",
        "for i in idx:\n",
        "    s = scores_test[i]\n",
        "    pred_labels = labels_from_row(s, thr, label_names)\n",
        "    top = topk(s, label_names, k=6)\n",
        "\n",
        "    row = {\n",
        "        \"row_idx\": int(i),\n",
        "        \"pred_n\": len(pred_labels),\n",
        "        \"pred_labels\": \"; \".join(pred_labels[:10]) + (\" ...\" if len(pred_labels) > 10 else \"\"),\n",
        "        \"top6\": \"; \".join([f\"{a}:{b:.3f}\" for a,b in top]),\n",
        "    }\n",
        "    if ids_test is not None:\n",
        "        # odatda primaryid bor bo‘ladi\n",
        "        for key in [\"primaryid\",\"caseid\",\"caseversion\"]:\n",
        "            if key in ids_test.columns:\n",
        "                row[key] = ids_test.loc[i, key]\n",
        "    rows.append(row)\n",
        "\n",
        "df_preview = pd.DataFrame(rows)\n",
        "print(tabulate(df_preview, headers=\"keys\", tablefmt=\"github\", showindex=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6) Raw CSV (zip) dan text/metadata ko‘rsatish (ixtiyoriy)\n",
        "#   - Siz upload qilgan: faers_25Q4_targets_multilabel_v2.zip\n",
        "#   - Agar projectda Data/Raw_data ichida bo‘lsa ham ishlaydi\n",
        "# =========================\n",
        "RAW_ZIP_CANDIDATES = [\n",
        "    PROJECT_ROOT / \"Data\" / \"Raw_data\" / \"faers_25Q4_targets_multilabel_v2.zip\",\n",
        "    PROJECT_ROOT / \"Data\" / \"Raw_data\" / \"faers_25Q4_targets_multilabel_v2.csv\",\n",
        "    Path(\"faers_25Q4_targets_multilabel_v2.zip\"),\n",
        "    Path(\"faers_25Q4_targets_multilabel_v2.csv\"),\n",
        "]\n",
        "\n",
        "raw_path = None\n",
        "for p in RAW_ZIP_CANDIDATES:\n",
        "    if p.exists():\n",
        "        raw_path = p\n",
        "        break\n",
        "\n",
        "raw_df = None\n",
        "if raw_path is None:\n",
        "    print(\"Raw data topilmadi (ok). Agar kerak bo‘lsa zip/csv ni Data/Raw_data ga qo‘ying.\")\n",
        "else:\n",
        "    print(\"Raw path:\", raw_path.resolve())\n",
        "    if raw_path.suffix.lower() == \".csv\":\n",
        "        raw_df = pd.read_csv(raw_path)\n",
        "    else:\n",
        "        with zipfile.ZipFile(raw_path) as zf:\n",
        "            # zip ichidagi 1chi csv\n",
        "            name = [n for n in zf.namelist() if n.lower().endswith(\".csv\")][0]\n",
        "            with zf.open(name) as f:\n",
        "                raw_df = pd.read_csv(f)\n",
        "\n",
        "    print(\"raw_df:\", raw_df.shape)\n",
        "\n",
        "# Agar raw_df bo‘lsa: preview jadvalidagi primaryid bo‘yicha text ko‘rsatamiz\n",
        "if raw_df is not None and ids_test is not None and \"primaryid\" in ids_test.columns and \"primaryid\" in raw_df.columns:\n",
        "    # 1) birinchi preview row primaryid'ni olamiz\n",
        "    pid = df_preview.iloc[0].get(\"primaryid\", None)\n",
        "    if pid is not None:\n",
        "        one = raw_df[raw_df[\"primaryid\"] == pid]\n",
        "        if len(one) > 0:\n",
        "            one = one.iloc[0]\n",
        "            # text col tanlash\n",
        "            candidates = [\"REAC_pt_symptom_v2\",\"REAC_pt_symptom\",\"REAC_pt\",\"REAC_pt_symptom_v2\"]\n",
        "            text_col = next((c for c in candidates if c in raw_df.columns), TEXT_COL_FALLBACK)\n",
        "            print(\"\\n--- RAW VIEW (1 sample) ---\")\n",
        "            cols_show = [c for c in [\"primaryid\",\"caseid\",\"caseversion\",\"DRUG_drugname\",\"DRUG_prod_ai\",text_col,\"y_labels\"] if c in raw_df.columns]\n",
        "            print(one[cols_show])\n",
        "        else:\n",
        "            print(\"raw_df ichida bu primaryid topilmadi:\", pid)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7) Single lookup (primaryid yoki row index)\n",
        "# =========================\n",
        "# 1) primaryid bilan qidirish:\n",
        "QUERY_PRIMARYID = None  # masalan: 1012809821\n",
        "\n",
        "# 2) yoki to‘g‘ridan-to‘g‘ri row_idx:\n",
        "QUERY_ROW_IDX = None    # masalan: 123\n",
        "\n",
        "def show_one_by_row(i: int):\n",
        "    s = scores_test[i]\n",
        "    pred_labels = labels_from_row(s, thr, label_names)\n",
        "    top = topk(s, label_names, k=12)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"OFFLINE PREDICTION\")\n",
        "    print(\"=\"*90)\n",
        "    if ids_test is not None:\n",
        "        meta_cols = [c for c in [\"primaryid\",\"caseid\",\"caseversion\"] if c in ids_test.columns]\n",
        "        if meta_cols:\n",
        "            print(\"IDs:\", ids_test.loc[i, meta_cols].to_dict())\n",
        "    print(\"pred_n:\", len(pred_labels))\n",
        "    print(\"pred_labels:\", \"; \".join(pred_labels))\n",
        "    print(\"top12:\", \"; \".join([f\"{a}:{b:.4f}\" for a,b in top]))\n",
        "\n",
        "    if Y_test is not None:\n",
        "        true_labels = [label_names[j] for j in range(len(label_names)) if Y_test[i, j] == 1]\n",
        "        print(\"true_n:\", len(true_labels))\n",
        "        print(\"true_labels:\", \"; \".join(true_labels))\n",
        "\n",
        "# primaryid -> row\n",
        "if QUERY_PRIMARYID is not None and ids_test is not None and \"primaryid\" in ids_test.columns:\n",
        "    hits = ids_test.index[ids_test[\"primaryid\"] == QUERY_PRIMARYID].tolist()\n",
        "    if not hits:\n",
        "        print(\"primaryid topilmadi:\", QUERY_PRIMARYID)\n",
        "    else:\n",
        "        show_one_by_row(int(hits[0]))\n",
        "elif QUERY_ROW_IDX is not None:\n",
        "    show_one_by_row(int(QUERY_ROW_IDX))\n",
        "else:\n",
        "    print(\"QUERY_PRIMARYID yoki QUERY_ROW_IDX bering (ixtiyoriy).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done\n",
        "Agar keyingi bosqichda siz **yangi (unseen) raw record** dan to‘g‘ridan-to‘g‘ri predict qilishni xohlasangiz (X_test.npz bo‘lmagan holat),\n",
        "unda preprocessing artefaktlari (TFIDF vectorizer + feature selector) ham saqlangan bo‘lishi kerak.\n",
        "Aytasiz — men shu notebookga o‘sha qismini ham aniq qo‘shib beraman (faqat sizda artefaktlar qayerda ekanini bilishimiz kerak)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}