{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 16) Compare Results â€” Baseline vs Improvement vs Tuning vs Best Model\n",
        "Bu notebook:\n",
        "- 06_train_baseline.ipynb\n",
        "- 10_train_improvement.ipynb\n",
        "- 12_tuning_NO_OVERSAMPLING.ipynb\n",
        "notebooklaridan natijalarni **avtomatik parse qilib**, bosqichma-bosqich (tagma-tag) koâ€˜rsatadi.\n",
        "\n",
        "**Ranglar:** har bir bosqich jadvalida har bir metrikaning **eng yuqori** qiymati yashil, **eng past** qiymati qizil.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'baseline': WindowsPath('c:/Users/xolmu/OneDrive/Desktop/Modul Program oyi/Modul_Program3/6_project_dori_tasiri_extract/Notebooks/06_train_baseline.ipynb'),\n",
              " 'improvement': WindowsPath('c:/Users/xolmu/OneDrive/Desktop/Modul Program oyi/Modul_Program3/6_project_dori_tasiri_extract/Notebooks/10_train_improvement.ipynb'),\n",
              " 'tuning': WindowsPath('c:/Users/xolmu/OneDrive/Desktop/Modul Program oyi/Modul_Program3/6_project_dori_tasiri_extract/Notebooks/12_tuning_NO_OVERSAMPLING.ipynb')}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from tabulate import tabulate\n",
        "except Exception:\n",
        "    tabulate = None\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "# ==========================\n",
        "# 0) Fayllarni topish\n",
        "# ==========================\n",
        "HERE = Path.cwd()\n",
        "\n",
        "def find_file(filename: str, root: Path = HERE) -> Path:\n",
        "    hits = list(root.rglob(filename))\n",
        "    if hits:\n",
        "        return hits[0]\n",
        "    # /mnt/data kabi joylarda ishlaganda ham yordam berish uchun:\n",
        "    alt_root = Path(\"/mnt/data\")\n",
        "    if alt_root.exists():\n",
        "        hits = list(alt_root.rglob(filename))\n",
        "        if hits:\n",
        "            return hits[0]\n",
        "    raise FileNotFoundError(f\"Topilmadi: {filename}. Compare notebook joylashgan papkada yoki uning ichida boâ€˜lishi kerak.\")\n",
        "\n",
        "FILES = {\n",
        "    \"baseline\": \"06_train_baseline.ipynb\",\n",
        "    \"improvement\": \"10_train_improvement.ipynb\",\n",
        "    \"tuning\": \"12_tuning_NO_OVERSAMPLING.ipynb\",\n",
        "}\n",
        "\n",
        "paths = {k: find_file(v) for k, v in FILES.items()}\n",
        "paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 1) Notebook outputlarini oâ€˜qish\n",
        "# ==========================\n",
        "def load_ipynb(path: Path) -> dict:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def iter_cell_text_outputs(nb: dict):\n",
        "    \"\"\"Yield (cell_index, text) for all stdout/stderr and text/plain outputs.\"\"\"\n",
        "    for ci, cell in enumerate(nb.get(\"cells\", [])):\n",
        "        if cell.get(\"cell_type\") != \"code\":\n",
        "            continue\n",
        "        # stream outputs\n",
        "        for out in cell.get(\"outputs\", []):\n",
        "            ot = out.get(\"output_type\")\n",
        "            if ot == \"stream\":\n",
        "                txt = out.get(\"text\", \"\")\n",
        "                if isinstance(txt, list):\n",
        "                    txt = \"\".join(txt)\n",
        "                yield ci, txt\n",
        "            elif ot in (\"execute_result\", \"display_data\"):\n",
        "                data = out.get(\"data\", {})\n",
        "                txt = data.get(\"text/plain\")\n",
        "                if txt is None:\n",
        "                    continue\n",
        "                if isinstance(txt, list):\n",
        "                    txt = \"\".join(txt)\n",
        "                yield ci, txt\n",
        "\n",
        "def get_cell_source(nb: dict, idx: int) -> str:\n",
        "    src = nb[\"cells\"][idx].get(\"source\", \"\")\n",
        "    return \"\".join(src) if isinstance(src, list) else str(src)\n",
        "\n",
        "def to_float(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 2) Parsers\n",
        "# ==========================\n",
        "RE_TEST_METRICS = re.compile(\n",
        "    r\"TEST metrics:\\s*\\n\\s*micro_f1:\\s*(?P<micro_f1>[0-9eE.+-]+)\\s*\\n\\s*macro_f1:\\s*(?P<macro_f1>[0-9eE.+-]+)\\s*\\n\\s*micro_P\\s*:\\s*(?P<micro_p>[0-9eE.+-]+)\\s*\\n\\s*micro_R\\s*:\\s*(?P<micro_r>[0-9eE.+-]+)\",\n",
        "    re.MULTILINE\n",
        ")\n",
        "\n",
        "RE_VAL_METRICS_IMPR = re.compile(\n",
        "    r\"TRAIN:\\s*(?P<model>\\S+)\\s*\\n=+\\s*\\n.*?VAL micro_f1:\\s*(?P<micro_f1>[0-9eE.+-]+)\\s*\\|\\s*macro_f1:\\s*(?P<macro_f1>[0-9eE.+-]+)\",\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "RE_TUPLE_VAL_TEST = re.compile(\n",
        "    r\"\\(\\{'micro_f1':\\s*(?P<val_micro>[0-9eE.+-]+),\\s*'macro_f1':\\s*(?P<val_macro>[0-9eE.+-]+)\\}\\s*,\\s*\\{'micro_f1':\\s*(?P<test_micro>[0-9eE.+-]+),\\s*'macro_f1':\\s*(?P<test_macro>[0-9eE.+-]+)\\}\\)\",\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "RE_FINAL_VAL = re.compile(r\"VAL:\\s*\\{[^}]*'micro_f1':\\s*(?P<val_micro>[0-9eE.+-]+),\\s*'macro_f1':\\s*(?P<val_macro>[0-9eE.+-]+)\\}\")\n",
        "RE_FINAL_TEST = re.compile(r\"TEST:\\s*\\{[^}]*'micro_f1':\\s*(?P<test_micro>[0-9eE.+-]+),\\s*'macro_f1':\\s*(?P<test_macro>[0-9eE.+-]+)\\}\")\n",
        "\n",
        "def parse_baseline(nb: dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for ci, txt in iter_cell_text_outputs(nb):\n",
        "        m = RE_TEST_METRICS.search(txt)\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        # model name: try run_name in source, else heuristic by cell index\n",
        "        src = get_cell_source(nb, ci)\n",
        "        rn = None\n",
        "        m_rn = re.search(r\"run_name\\s*=\\s*['\\\"](?P<rn>[^'\\\"]+)['\\\"]\", src)\n",
        "        if m_rn:\n",
        "            rn = m_rn.group(\"rn\")\n",
        "        else:\n",
        "            # LogReg baseline evaluate cell\n",
        "            rn = \"baseline_ovr_logreg\" if ci == 13 else f\"baseline_cell_{ci}\"\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": rn,\n",
        "            \"test_micro_f1\": to_float(m.group(\"micro_f1\")),\n",
        "            \"test_macro_f1\": to_float(m.group(\"macro_f1\")),\n",
        "            \"test_micro_precision\": to_float(m.group(\"micro_p\")),\n",
        "            \"test_micro_recall\": to_float(m.group(\"micro_r\")),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"model\"], keep=\"first\")\n",
        "    # Keng tarqalgan tartib\n",
        "    if not df.empty:\n",
        "        df = df.sort_values([\"test_micro_f1\",\"test_macro_f1\"], ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def parse_improvement(nb: dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    # bu notebookda odatda har model bitta cell stdout'ida turadi\n",
        "    for ci, txt in iter_cell_text_outputs(nb):\n",
        "        if \"TRAIN:\" not in txt or \"VAL micro_f1\" not in txt:\n",
        "            continue\n",
        "        m = RE_VAL_METRICS_IMPR.search(txt)\n",
        "        if not m:\n",
        "            continue\n",
        "        rows.append({\n",
        "            \"model\": m.group(\"model\").strip(),\n",
        "            \"val_micro_f1\": to_float(m.group(\"micro_f1\")),\n",
        "            \"val_macro_f1\": to_float(m.group(\"macro_f1\")),\n",
        "        })\n",
        "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"model\"], keep=\"first\")\n",
        "    if not df.empty:\n",
        "        df = df.sort_values([\"val_micro_f1\",\"val_macro_f1\"], ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def parse_tuning(nb: dict) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    algo_by_hint = {\n",
        "        \"logreg_optuna\": \"tune_optuna_logreg\",\n",
        "        \"linearsvc_optuna\": \"tune_optuna_linearsvc\",\n",
        "        \"sgd_logloss_optuna\": \"tune_optuna_sgd_logloss\",\n",
        "        \"sgd_hinge_optuna\": \"tune_optuna_sgd_hinge\",\n",
        "    }\n",
        "    for ci, txt in iter_cell_text_outputs(nb):\n",
        "        m = RE_TUPLE_VAL_TEST.search(txt)\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        # algo hint: oldingi log satrida study name boâ€˜ladi\n",
        "        model = f\"tune_cell_{ci}\"\n",
        "        for hint, name in algo_by_hint.items():\n",
        "            if hint in txt:\n",
        "                model = name\n",
        "                break\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": model,\n",
        "            \"val_micro_f1\": to_float(m.group(\"val_micro\")),\n",
        "            \"val_macro_f1\": to_float(m.group(\"val_macro\")),\n",
        "            \"test_micro_f1\": to_float(m.group(\"test_micro\")),\n",
        "            \"test_macro_f1\": to_float(m.group(\"test_macro\")),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"model\"], keep=\"first\")\n",
        "    if not df.empty:\n",
        "        # tuning jadvalini test boâ€˜yicha tartiblaymiz\n",
        "        df = df.sort_values([\"test_micro_f1\",\"test_macro_f1\"], ascending=False).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def parse_best_model(nb: dict) -> pd.DataFrame:\n",
        "    # FINAL TRAIN cell stdoutâ€™idan olamiz\n",
        "    best = None\n",
        "    for ci, txt in iter_cell_text_outputs(nb):\n",
        "        if \"Training LogisticRegression\" in txt and \"VAL:\" in txt and \"TEST:\" in txt:\n",
        "            mv = RE_FINAL_VAL.search(txt)\n",
        "            mt = RE_FINAL_TEST.search(txt)\n",
        "            if mv and mt:\n",
        "                best = {\n",
        "                    \"model\": \"optuna_logreg_best_final\",\n",
        "                    \"val_micro_f1\": to_float(mv.group(\"val_micro\")),\n",
        "                    \"val_macro_f1\": to_float(mv.group(\"val_macro\")),\n",
        "                    \"test_micro_f1\": to_float(mt.group(\"test_micro\")),\n",
        "                    \"test_macro_f1\": to_float(mt.group(\"test_macro\")),\n",
        "                }\n",
        "                break\n",
        "    df = pd.DataFrame([best]) if best else pd.DataFrame(columns=[\n",
        "        \"model\",\"val_micro_f1\",\"val_macro_f1\",\"test_micro_f1\",\"test_macro_f1\"\n",
        "    ])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(                               model  test_micro_f1  test_macro_f1  \\\n",
              " 0             baseline_ovr_linearsvc         0.9995         0.9988   \n",
              " 1  baseline_ovr_calibrated_linearsvc         0.9992         0.9984   \n",
              " 2                baseline_ovr_logreg         0.9973         0.9958   \n",
              " 3           baseline_ovr_sgd_logloss         0.9950         0.9920   \n",
              " \n",
              "    test_micro_precision  test_micro_recall  \n",
              " 0                0.9992             0.9998  \n",
              " 1                0.9989             0.9995  \n",
              " 2                0.9960             0.9986  \n",
              " 3                0.9934             0.9966  ,\n",
              "                   model  val_micro_f1  val_macro_f1\n",
              " 0     ovr_logreg_bal_C2      0.980376      0.965836\n",
              " 1      ovr_linearsvc_C1      0.980182      0.964417\n",
              " 2     ovr_logreg_bal_C1      0.978403      0.944185\n",
              " 3       ovr_sgd_logloss      0.974033      0.952951\n",
              " 4         ovr_sgd_hinge      0.957535      0.936905\n",
              " 5  ovr_complementnb_a05      0.913360      0.882115,\n",
              "          model  val_micro_f1  val_macro_f1  test_micro_f1  test_macro_f1\n",
              " 0  tune_cell_4      0.962941      0.930969       0.957416       0.916767\n",
              " 1  tune_cell_5      0.961265      0.908998       0.957016       0.912317\n",
              " 2  tune_cell_6      0.950963      0.916381       0.943415       0.903986\n",
              " 3  tune_cell_7      0.948021      0.888889       0.938087       0.895398,\n",
              "                       model  val_micro_f1  val_macro_f1  test_micro_f1  \\\n",
              " 0  optuna_logreg_best_final      0.984831      0.973785       0.978199   \n",
              " \n",
              "    test_macro_f1  \n",
              " 0       0.966022  )"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==========================\n",
        "# 3) Parse + DataFrames\n",
        "# ==========================\n",
        "baseline_nb = load_ipynb(paths[\"baseline\"])\n",
        "impr_nb = load_ipynb(paths[\"improvement\"])\n",
        "tune_nb = load_ipynb(paths[\"tuning\"])\n",
        "\n",
        "df_baseline = parse_baseline(baseline_nb)\n",
        "df_impr = parse_improvement(impr_nb)\n",
        "df_tune = parse_tuning(tune_nb)\n",
        "df_best = parse_best_model(tune_nb)\n",
        "\n",
        "df_baseline, df_impr, df_tune, df_best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 4) Tabulate + ðŸŸ©/ðŸŸ¥ (max/min) â€” jinja2 kerak EMAS\n",
        "# ==========================\n",
        "\n",
        "def add_max_min_marks(df: pd.DataFrame, cols):\n",
        "    \"\"\"Return a copy where each numeric value is formatted and tagged with ðŸŸ©/ðŸŸ¥ within cols (per-column).\"\"\"\n",
        "    out = df.copy()\n",
        "    for c in cols:\n",
        "        if c not in out.columns:\n",
        "            continue\n",
        "        s = pd.to_numeric(out[c], errors=\"coerce\")\n",
        "        if s.isna().all():\n",
        "            continue\n",
        "        vmax = s.max()\n",
        "        vmin = s.min()\n",
        "\n",
        "        def fmt(v):\n",
        "            if pd.isna(v):\n",
        "                return \"â€”\"\n",
        "            v = float(v)\n",
        "            txt = f\"{v:.6f}\"\n",
        "            if v == vmax and v == vmin:\n",
        "                return txt + \"ðŸŸ©ðŸŸ¥\"\n",
        "            if v == vmax:\n",
        "                return txt + \"ðŸŸ©\"\n",
        "            if v == vmin:\n",
        "                return txt + \"ðŸŸ¥\"\n",
        "            return txt\n",
        "\n",
        "        out[c] = s.map(fmt)\n",
        "    return out\n",
        "\n",
        "def show_stage(stage_name: str, df: pd.DataFrame, mark_cols):\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(stage_name)\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"(Bu bosqich uchun parse qilingan natija topilmadi.)\")\n",
        "        return\n",
        "\n",
        "    df2 = add_max_min_marks(df, mark_cols)\n",
        "\n",
        "    if tabulate is not None:\n",
        "        print(tabulate(df2, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
        "    else:\n",
        "        print(df2.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "BASELINE (from 06_train_baseline.ipynb) â€” TEST metrics\n",
            "==========================================================================================\n",
            "| model                             | test_micro_f1   | test_macro_f1   | test_micro_precision   | test_micro_recall   |\n",
            "|-----------------------------------|-----------------|-----------------|------------------------|---------------------|\n",
            "| baseline_ovr_linearsvc            | 0.999500ðŸŸ©      | 0.998800ðŸŸ©      | 0.999200ðŸŸ©             | 0.999800ðŸŸ©          |\n",
            "| baseline_ovr_calibrated_linearsvc | 0.999200        | 0.998400        | 0.998900               | 0.999500            |\n",
            "| baseline_ovr_logreg               | 0.997300        | 0.995800        | 0.996000               | 0.998600            |\n",
            "| baseline_ovr_sgd_logloss          | 0.995000ðŸŸ¥      | 0.992000ðŸŸ¥      | 0.993400ðŸŸ¥             | 0.996600ðŸŸ¥          |\n"
          ]
        }
      ],
      "source": [
        "show_stage('BASELINE (from 06_train_baseline.ipynb) â€” TEST metrics', df_baseline,\n",
        "          ['test_micro_f1','test_macro_f1','test_micro_precision','test_micro_recall'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improvement results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "IMPROVEMENT (from 10_train_improvement.ipynb) â€” VAL metrics\n",
            "==========================================================================================\n",
            "| model                | val_micro_f1   | val_macro_f1   |\n",
            "|----------------------|----------------|----------------|\n",
            "| ovr_logreg_bal_C2    | 0.980376ðŸŸ©     | 0.965836ðŸŸ©     |\n",
            "| ovr_linearsvc_C1     | 0.980182       | 0.964417       |\n",
            "| ovr_logreg_bal_C1    | 0.978403       | 0.944185       |\n",
            "| ovr_sgd_logloss      | 0.974033       | 0.952951       |\n",
            "| ovr_sgd_hinge        | 0.957535       | 0.936905       |\n",
            "| ovr_complementnb_a05 | 0.913360ðŸŸ¥     | 0.882115ðŸŸ¥     |\n"
          ]
        }
      ],
      "source": [
        "show_stage('IMPROVEMENT (from 10_train_improvement.ipynb) â€” VAL metrics', df_impr,\n",
        "          ['val_micro_f1','val_macro_f1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning results (Optuna, NO_OVERSAMPLING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "TUNING (from 12_tuning_NO_OVERSAMPLING.ipynb) â€” VAL/TEST metrics\n",
            "==========================================================================================\n",
            "| model       | val_micro_f1   | val_macro_f1   | test_micro_f1   | test_macro_f1   |\n",
            "|-------------|----------------|----------------|-----------------|-----------------|\n",
            "| tune_cell_4 | 0.962941ðŸŸ©     | 0.930969ðŸŸ©     | 0.957416ðŸŸ©      | 0.916767ðŸŸ©      |\n",
            "| tune_cell_5 | 0.961265       | 0.908998       | 0.957016        | 0.912317        |\n",
            "| tune_cell_6 | 0.950963       | 0.916381       | 0.943415        | 0.903986        |\n",
            "| tune_cell_7 | 0.948021ðŸŸ¥     | 0.888889ðŸŸ¥     | 0.938087ðŸŸ¥      | 0.895398ðŸŸ¥      |\n"
          ]
        }
      ],
      "source": [
        "show_stage('TUNING (from 12_tuning_NO_OVERSAMPLING.ipynb) â€” VAL/TEST metrics', df_tune,\n",
        "          ['val_micro_f1','val_macro_f1','test_micro_f1','test_macro_f1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best model (Final Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "BEST MODEL (FINAL TRAIN in 12_tuning_NO_OVERSAMPLING.ipynb) â€” VAL/TEST metrics\n",
            "==========================================================================================\n",
            "| model                    | val_micro_f1   | val_macro_f1   | test_micro_f1   | test_macro_f1   |\n",
            "|--------------------------|----------------|----------------|-----------------|-----------------|\n",
            "| optuna_logreg_best_final | 0.984831ðŸŸ©ðŸŸ¥   | 0.973785ðŸŸ©ðŸŸ¥   | 0.978199ðŸŸ©ðŸŸ¥    | 0.966022ðŸŸ©ðŸŸ¥    |\n"
          ]
        }
      ],
      "source": [
        "show_stage('BEST MODEL (FINAL TRAIN in 12_tuning_NO_OVERSAMPLING.ipynb) â€” VAL/TEST metrics', df_best,\n",
        "          ['val_micro_f1','val_macro_f1','test_micro_f1','test_macro_f1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary â€” best-of-stage comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "SUMMARY (Best row from each stage)\n",
            "==========================================================================================\n",
            "| stage       | model                    | val_micro_f1   | val_macro_f1   | test_micro_f1   | test_macro_f1   | test_micro_precision   | test_micro_recall   |\n",
            "|-------------|--------------------------|----------------|----------------|-----------------|-----------------|------------------------|---------------------|\n",
            "| baseline    | baseline_ovr_linearsvc   | â€”              | â€”              | 0.999500ðŸŸ©      | 0.998800ðŸŸ©      | 0.999200ðŸŸ©ðŸŸ¥           | 0.999800ðŸŸ©ðŸŸ¥        |\n",
            "| improvement | ovr_logreg_bal_C2        | 0.980376       | 0.965836       | â€”               | â€”               | â€”                      | â€”                   |\n",
            "| tuning      | tune_cell_4              | 0.962941ðŸŸ¥     | 0.930969ðŸŸ¥     | 0.957416ðŸŸ¥      | 0.916767ðŸŸ¥      | â€”                      | â€”                   |\n",
            "| best_model  | optuna_logreg_best_final | 0.984831ðŸŸ©     | 0.973785ðŸŸ©     | 0.978199        | 0.966022        | â€”                      | â€”                   |\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# 5) Best-of-stage summary\n",
        "# ==========================\n",
        "def pick_best(df: pd.DataFrame, prefer_test: bool = True):\n",
        "    if df.empty:\n",
        "        return None\n",
        "    if prefer_test and \"test_micro_f1\" in df.columns:\n",
        "        col = \"test_micro_f1\"\n",
        "    elif \"val_micro_f1\" in df.columns:\n",
        "        col = \"val_micro_f1\"\n",
        "    else:\n",
        "        # fallback: first numeric col\n",
        "        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "        col = num_cols[0] if num_cols else None\n",
        "    if col is None:\n",
        "        return None\n",
        "    return df.sort_values(col, ascending=False).iloc[0].to_dict()\n",
        "\n",
        "best_rows = []\n",
        "b = pick_best(df_baseline, prefer_test=True)\n",
        "if b: b[\"stage\"] = \"baseline\"; best_rows.append(b)\n",
        "\n",
        "i = pick_best(df_impr, prefer_test=False)\n",
        "if i: i[\"stage\"] = \"improvement\"; best_rows.append(i)\n",
        "\n",
        "t = pick_best(df_tune, prefer_test=True)\n",
        "if t: t[\"stage\"] = \"tuning\"; best_rows.append(t)\n",
        "\n",
        "bm = pick_best(df_best, prefer_test=True)\n",
        "if bm: bm[\"stage\"] = \"best_model\"; best_rows.append(bm)\n",
        "\n",
        "summary = pd.DataFrame(best_rows)\n",
        "\n",
        "# Ustunlarni chiroyli tartibda chiqaramiz\n",
        "wanted = [\"stage\",\"model\",\n",
        "          \"val_micro_f1\",\"val_macro_f1\",\n",
        "          \"test_micro_f1\",\"test_macro_f1\",\n",
        "          \"test_micro_precision\",\"test_micro_recall\"]\n",
        "cols = [c for c in wanted if c in summary.columns] + [c for c in summary.columns if c not in wanted]\n",
        "summary = summary[cols]\n",
        "\n",
        "show_stage(\"SUMMARY (Best row from each stage)\", summary, [c for c in summary.columns if c not in ('stage','model')])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1a00f44a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfidf_vectorizer.joblib -> 0\n",
            "feature_selector.joblib -> 0\n",
            "*vectorizer*.joblib -> 0\n",
            "*selector*.joblib -> 0\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "root = Path(\".\")  # project rootda turgan boâ€˜lsank\n",
        "patterns = [\"tfidf_vectorizer.joblib\", \"feature_selector.joblib\", \"*vectorizer*.joblib\", \"*selector*.joblib\"]\n",
        "\n",
        "for pat in patterns:\n",
        "    hits = list(root.rglob(pat))\n",
        "    print(pat, \"->\", len(hits))\n",
        "    for h in hits[:20]:\n",
        "        print(\"  \", h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a4dc8a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_folder",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
