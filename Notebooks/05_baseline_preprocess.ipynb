{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf37e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n",
      "SPLIT_DIR exists: True c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Processed\\splits_multilabel_noleakage\n",
      "OUT_DIR: c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Preprocessed_data\\baseline\n"
     ]
    }
   ],
   "source": [
    "# CELL 1 — Imports + Paths\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# --- Project root topish ---\n",
    "CWD = Path.cwd()\n",
    "if (CWD / \"Data\").exists():\n",
    "    PROJECT_ROOT = CWD\n",
    "elif (CWD.parent / \"Data\").exists():\n",
    "    PROJECT_ROOT = CWD.parent\n",
    "else:\n",
    "    PROJECT_ROOT = CWD\n",
    "\n",
    "# --- Sizning split papkangiz (aniq nom shu bo'lsin) ---\n",
    "SPLIT_DIR = PROJECT_ROOT / \"Data\" / \"Processed\" / \"splits_multilabel_noleakage\"\n",
    "\n",
    "# --- Preprocess output ---\n",
    "OUT_DIR = PROJECT_ROOT / \"Data\" / \"Preprocessed_data\" / \"baseline\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"SPLIT_DIR exists:\", SPLIT_DIR.exists(), SPLIT_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f715723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (201176, 24) (24410, 24) (24164, 24)\n",
      "Num labels: 21\n",
      "First labels: ['y_cardiovascular', 'y_dermatologic', 'y_edema_swelling', 'y_gastrointestinal', 'y_general_systemic', 'y_hematologic', 'y_hepatic', 'y_hypersensitivity_allergy']\n",
      "train: empty_text=0 | zero_label=0\n",
      "val: empty_text=0 | zero_label=0\n",
      "test: empty_text=0 | zero_label=0\n"
     ]
    }
   ],
   "source": [
    "# CELL 2 — Load train/val/test + Columns\n",
    "TRAIN_CSV = SPLIT_DIR / \"train.csv\"\n",
    "VAL_CSV   = SPLIT_DIR / \"val.csv\"\n",
    "TEST_CSV  = SPLIT_DIR / \"test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV, low_memory=False)\n",
    "df_val   = pd.read_csv(VAL_CSV, low_memory=False)\n",
    "df_test  = pd.read_csv(TEST_CSV, low_memory=False)\n",
    "\n",
    "TEXT_COL = \"REAC_pt_symptom_v2\"\n",
    "if TEXT_COL not in df_train.columns:\n",
    "    raise ValueError(f\"TEXT_COL topilmadi: {TEXT_COL}\")\n",
    "\n",
    "y_cols = sorted([c for c in df_train.columns if c.startswith(\"y_\") and c != \"y_labels\"])\n",
    "if len(y_cols) == 0:\n",
    "    raise ValueError(\"y_ label ustunlari topilmadi.\")\n",
    "\n",
    "print(\"Shapes:\", df_train.shape, df_val.shape, df_test.shape)\n",
    "print(\"Num labels:\", len(y_cols))\n",
    "print(\"First labels:\", y_cols[:8])\n",
    "\n",
    "# sanity\n",
    "for name, dfx in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    empty_txt = (dfx[TEXT_COL].fillna(\"\").astype(str).str.strip() == \"\").sum()\n",
    "    zero_lbl  = (dfx[y_cols].sum(axis=1) == 0).sum()\n",
    "    print(f\"{name}: empty_text={int(empty_txt)} | zero_label={int(zero_lbl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc96341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text: injection site reaction general physical health deterioration chest discomfort sensitivity to weather change fatigue dysphonia wheezing pain influenza productive cough nasopharyngitis weight decreased nasal congestion hypoventilation illness blood pressure decreased forced expiratory volume decreased hypersensitivity dyspnoea body temperature decreased\n",
      "Y shapes: (201176, 21) (24410, 21) (24164, 21)\n"
     ]
    }
   ],
   "source": [
    "# Text normalize + X/Y tayyorlash\n",
    "\n",
    "# Bu baseline uchun yetarli va user erkin yozadigan matnga yaqin qiladi.\n",
    "\n",
    "_ws = re.compile(r\"\\s+\")\n",
    "_punct = re.compile(r\"[,\\t\\r\\n]+\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.lower().strip()\n",
    "    # FAERS PT'lar \";\" bilan keladi — user matniga yaqinlashtirish uchun space qilamiz\n",
    "    s = s.replace(\";\", \" \")\n",
    "    s = _punct.sub(\" \", s)\n",
    "    s = _ws.sub(\" \", s)\n",
    "    return s\n",
    "\n",
    "Xtr_text = df_train[TEXT_COL].map(normalize_text).values\n",
    "Xva_text = df_val[TEXT_COL].map(normalize_text).values\n",
    "Xte_text = df_test[TEXT_COL].map(normalize_text).values\n",
    "\n",
    "# Y (multi-label)\n",
    "Ytr = df_train[y_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(np.int8).values\n",
    "Yva = df_val[y_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(np.int8).values\n",
    "Yte = df_test[y_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(np.int8).values\n",
    "\n",
    "print(\"Example text:\", Xtr_text[0])\n",
    "print(\"Y shapes:\", Ytr.shape, Yva.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7e9264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Fit/transform CHAR TF-IDF ...\n",
      "[2/2] Fit/transform WORD TF-IDF ...\n",
      "X shapes: (201176, 58921) (24410, 58921) (24164, 58921)\n",
      "nnz train: 27079723\n"
     ]
    }
   ],
   "source": [
    "#CELL 4 — TF-IDF (Char + Word) → Combine (BEST baseline)\n",
    "# Char TF-IDF: typo/imlo/sinonimga chidamli\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=3,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    dtype=np.float32,\n",
    "    max_features=180_000\n",
    ")\n",
    "\n",
    "# Word TF-IDF: interpretatsiya yaxshi (qaysi so'zlar signal)\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    dtype=np.float32,\n",
    "    max_features=90_000,\n",
    "    token_pattern=r\"(?u)\\b[a-z0-9][a-z0-9\\-\\_]+\\b\"\n",
    ")\n",
    "\n",
    "print(\"[1/2] Fit/transform CHAR TF-IDF ...\")\n",
    "Xtr_c = tfidf_char.fit_transform(Xtr_text)\n",
    "Xva_c = tfidf_char.transform(Xva_text)\n",
    "Xte_c = tfidf_char.transform(Xte_text)\n",
    "\n",
    "print(\"[2/2] Fit/transform WORD TF-IDF ...\")\n",
    "Xtr_w = tfidf_word.fit_transform(Xtr_text)\n",
    "Xva_w = tfidf_word.transform(Xva_text)\n",
    "Xte_w = tfidf_word.transform(Xte_text)\n",
    "\n",
    "# Combine\n",
    "Xtr = hstack([Xtr_c, Xtr_w], format=\"csr\")\n",
    "Xva = hstack([Xva_c, Xva_w], format=\"csr\")\n",
    "Xte = hstack([Xte_c, Xte_w], format=\"csr\")\n",
    "\n",
    "print(\"X shapes:\", Xtr.shape, Xva.shape, Xte.shape)\n",
    "print(\"nnz train:\", Xtr.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15102086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED => c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Preprocessed_data\\baseline\n",
      "{'text_col': 'REAC_pt_symptom_v2', 'num_labels': 21, 'num_features_char': 28926, 'num_features_word': 29995, 'num_features_total': 58921, 'rows_train': 201176, 'rows_val': 24410, 'rows_test': 24164, 'char_params': {'analyzer': 'char_wb', 'ngram_range': [3, 5], 'min_df': 3, 'max_df': 0.95, 'sublinear_tf': True, 'max_features': 180000, 'dtype': 'float32'}, 'word_params': {'analyzer': 'word', 'ngram_range': [1, 2], 'min_df': 5, 'max_df': 0.95, 'sublinear_tf': True, 'max_features': 90000, 'dtype': 'float32'}}\n"
     ]
    }
   ],
   "source": [
    "#Save (X sparse + Y + bundle + meta)\n",
    "# --- Save sparse matrices ---\n",
    "sparse.save_npz(OUT_DIR / \"X_train.npz\", Xtr)\n",
    "sparse.save_npz(OUT_DIR / \"X_val.npz\",   Xva)\n",
    "sparse.save_npz(OUT_DIR / \"X_test.npz\",  Xte)\n",
    "\n",
    "# --- Save labels ---\n",
    "np.save(OUT_DIR / \"Y_train.npy\", Ytr)\n",
    "np.save(OUT_DIR / \"Y_val.npy\",   Yva)\n",
    "np.save(OUT_DIR / \"Y_test.npy\",  Yte)\n",
    "\n",
    "# --- Save ids (foydali) ---\n",
    "if \"primaryid\" in df_train.columns:\n",
    "    np.save(OUT_DIR / \"id_train.npy\", df_train[\"primaryid\"].values)\n",
    "    np.save(OUT_DIR / \"id_val.npy\",   df_val[\"primaryid\"].values)\n",
    "    np.save(OUT_DIR / \"id_test.npy\",  df_test[\"primaryid\"].values)\n",
    "\n",
    "# --- Save vectorizer bundle ---\n",
    "bundle = {\n",
    "    \"text_col\": TEXT_COL,\n",
    "    \"y_cols\": y_cols,\n",
    "    \"tfidf_char\": tfidf_char,\n",
    "    \"tfidf_word\": tfidf_word,\n",
    "    \"version\": \"baseline_preprocess_v3_noleakage\",\n",
    "}\n",
    "joblib.dump(bundle, OUT_DIR / \"tfidf_bundle.joblib\")\n",
    "\n",
    "# --- Meta ---\n",
    "meta = {\n",
    "    \"text_col\": TEXT_COL,\n",
    "    \"num_labels\": int(len(y_cols)),\n",
    "    \"num_features_char\": int(len(tfidf_char.get_feature_names_out())),\n",
    "    \"num_features_word\": int(len(tfidf_word.get_feature_names_out())),\n",
    "    \"num_features_total\": int(Xtr.shape[1]),\n",
    "    \"rows_train\": int(Xtr.shape[0]),\n",
    "    \"rows_val\": int(Xva.shape[0]),\n",
    "    \"rows_test\": int(Xte.shape[0]),\n",
    "    \"char_params\": {\n",
    "        \"analyzer\": \"char_wb\", \"ngram_range\": [3,5], \"min_df\": 3, \"max_df\": 0.95,\n",
    "        \"sublinear_tf\": True, \"max_features\": 180000, \"dtype\": \"float32\"\n",
    "    },\n",
    "    \"word_params\": {\n",
    "        \"analyzer\": \"word\", \"ngram_range\": [1,2], \"min_df\": 5, \"max_df\": 0.95,\n",
    "        \"sublinear_tf\": True, \"max_features\": 90000, \"dtype\": \"float32\"\n",
    "    }\n",
    "}\n",
    "(OUT_DIR / \"preprocess_meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"SAVED =>\", OUT_DIR)\n",
    "print(meta)\n",
    "\n",
    "\n",
    "\n",
    "# Natijada Data/Processed/baseline_preprocess_v2/ ichida:\n",
    "\n",
    "# X_train.npz / X_val.npz / X_test.npz\n",
    "\n",
    "# Y_train.npy / Y_val.npy / Y_test.npy\n",
    "\n",
    "# id_train.npy / id_val.npy / id_test.npy (agar primaryid bo‘lsa)\n",
    "\n",
    "# tfidf_bundle.joblib\n",
    "\n",
    "# preprocess_meta.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed74c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Y values: {np.int8(0), np.int8(1)}\n",
      "Xtr has NaN: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>mean_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pain</td>\n",
       "      <td>0.025193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rash</td>\n",
       "      <td>0.019115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>site</td>\n",
       "      <td>0.016978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>infection</td>\n",
       "      <td>0.015799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diarrhoea</td>\n",
       "      <td>0.015617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>injection</td>\n",
       "      <td>0.015534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>injection site</td>\n",
       "      <td>0.015447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>increased</td>\n",
       "      <td>0.014365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>decreased</td>\n",
       "      <td>0.013348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nausea</td>\n",
       "      <td>0.013016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fatigue</td>\n",
       "      <td>0.012639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>disorder</td>\n",
       "      <td>0.012542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>haemorrhage</td>\n",
       "      <td>0.011828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dyspnoea</td>\n",
       "      <td>0.011820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>headache</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pneumonia</td>\n",
       "      <td>0.011505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dermatitis</td>\n",
       "      <td>0.011450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>arthralgia</td>\n",
       "      <td>0.010960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>blood</td>\n",
       "      <td>0.010747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>swelling</td>\n",
       "      <td>0.010111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              term  mean_tfidf\n",
       "0             pain    0.025193\n",
       "1             rash    0.019115\n",
       "2             site    0.016978\n",
       "3        infection    0.015799\n",
       "4        diarrhoea    0.015617\n",
       "5        injection    0.015534\n",
       "6   injection site    0.015447\n",
       "7        increased    0.014365\n",
       "8        decreased    0.013348\n",
       "9           nausea    0.013016\n",
       "10         fatigue    0.012639\n",
       "11        disorder    0.012542\n",
       "12     haemorrhage    0.011828\n",
       "13        dyspnoea    0.011820\n",
       "14        headache    0.011700\n",
       "15       pneumonia    0.011505\n",
       "16      dermatitis    0.011450\n",
       "17      arthralgia    0.010960\n",
       "18           blood    0.010747\n",
       "19        swelling    0.010111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CELL 6 — Quick “hammasi joyida” tekshiruv\n",
    "# 1) Y faqat 0/1 ekanini tekshiruv\n",
    "vals = set(np.unique(Ytr))\n",
    "print(\"Unique Y values:\", vals)\n",
    "\n",
    "# 2) X da NaN yo'qligi (sparse)\n",
    "print(\"Xtr has NaN:\", np.isnan(Xtr.data).any())\n",
    "\n",
    "# 3) Word featurelardan top 20 (interpretatsiya uchun)\n",
    "feat_word = tfidf_word.get_feature_names_out()\n",
    "mean_w = np.asarray(Xtr_w.mean(axis=0)).ravel()\n",
    "top_idx = mean_w.argsort()[::-1][:20]\n",
    "\n",
    "pd.DataFrame({\"term\": feat_word[top_idx], \"mean_tfidf\": mean_w[top_idx].round(6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af0c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd004dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105e3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6d582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83b15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53972896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9a6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fea9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_folder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
