{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad63beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n",
      "DATA_SOURCE: Feature_Selected\n",
      "DATA_DIR: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Feature_Selected\\fe_v1_fs_chi2_v1\n",
      "MODEL_SAVE_DIR: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\n",
      "RESULTS_DIR: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\results\\optuna_tuning\\optuna_20260226_005315\n",
      "X: (201176, 33671) (24410, 33671) (24164, 33671)\n",
      "Y: (201176, 21) (24410, 21) (24164, 21)\n",
      "labels: 21\n"
     ]
    }
   ],
   "source": [
    "# 4 ta algoritm (LogReg, LinearSVC, SGD log_loss, SGD hinge) uchun Optuna tuning qiladi va:\n",
    "\n",
    "# best paramsni topadi\n",
    "\n",
    "# best modelni qayta train qiladi\n",
    "\n",
    "# VAL’da per-label threshold tuning qiladi\n",
    "\n",
    "# hammasini saqlaydi:\n",
    "\n",
    "# ✅ Model: Models/improvement_models/Improvement_Models/Optuna_Tuned_<RUN_ID>/...\n",
    "# ✅ Threshold: ..._thresholds.json\n",
    "# ✅ Best params: ..._best_params.json\n",
    "# ✅ Trial results: results/optuna_tuning/<RUN_ID>/*.csv\n",
    "# ✅ Test summary: ..._test_summary.json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CELL 1 — Load X/Y (Feature_Selected dan) + paths (root adashmaydi)\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "VERSION = \"fe_v1_fs_chi2_v1\"      # Feature_Selected versiya\n",
    "PREFER_FEATURE_SELECTED = True\n",
    "\n",
    "N_THR = 31\n",
    "N_JOBS = 1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "N_TRIALS = 15         # tez boshlash (keyin 50/100)\n",
    "TIMEOUT_SEC = 1800    # xohlasangiz: 1800 (30 min) kabi\n",
    "\n",
    "RUN_ID = datetime.now().strftime(\"optuna_%Y%m%d_%H%M%S\")\n",
    "\n",
    "def find_project_root(start: Path | None = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for p in [start] + list(start.parents):\n",
    "        data = p / \"Data\"\n",
    "        raw = data / \"Raw_data\"\n",
    "        processed = data / \"Processed\"\n",
    "        ok = False\n",
    "        if raw.exists() and any(raw.iterdir()): ok = True\n",
    "        if processed.exists() and any(processed.iterdir()): ok = True\n",
    "        if ok:\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "def find_data_dir(project_root: Path, version: str, prefer_fs: bool = True) -> tuple[Path, str]:\n",
    "    fs = project_root / \"Data\" / \"Feature_Selected\" / version\n",
    "    eng = project_root / \"Data\" / \"Engineered_data\" / version\n",
    "    if prefer_fs and (fs / \"X_train.npz\").exists(): return fs, \"Feature_Selected\"\n",
    "    if (eng / \"X_train.npz\").exists(): return eng, \"Engineered_data\"\n",
    "    if (fs / \"X_train.npz\").exists(): return fs, \"Feature_Selected\"\n",
    "    raise FileNotFoundError(f\"X_train.npz topilmadi: {fs} yoki {eng}\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR, DATA_SOURCE = find_data_dir(PROJECT_ROOT, VERSION, PREFER_FEATURE_SELECTED)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT.resolve())\n",
    "print(\"DATA_SOURCE:\", DATA_SOURCE)\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
    "\n",
    "# outputs\n",
    "MODEL_SAVE_DIR = PROJECT_ROOT / \"Models\" / \"improvement_models\" / \"Improvement_Models\" / f\"Optuna_Tuned_{RUN_ID}\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\" / \"optuna_tuning\" / RUN_ID\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"MODEL_SAVE_DIR:\", MODEL_SAVE_DIR.resolve())\n",
    "print(\"RESULTS_DIR:\", RESULTS_DIR.resolve())\n",
    "\n",
    "with open(DATA_DIR / \"engineered_meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "y_cols = meta[\"y_cols\"]\n",
    "\n",
    "X_train = sparse.load_npz(DATA_DIR / \"X_train.npz\").tocsr()\n",
    "X_val   = sparse.load_npz(DATA_DIR / \"X_val.npz\").tocsr()\n",
    "X_test  = sparse.load_npz(DATA_DIR / \"X_test.npz\").tocsr()\n",
    "\n",
    "Y_train = np.load(DATA_DIR / \"Y_train.npy\")\n",
    "Y_val   = np.load(DATA_DIR / \"Y_val.npy\")\n",
    "Y_test  = np.load(DATA_DIR / \"Y_test.npy\")\n",
    "\n",
    "print(\"X:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Y:\", Y_train.shape, Y_val.shape, Y_test.shape)\n",
    "print(\"labels:\", len(y_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8533f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 — Helper: score/threshold/metric (Optuna objective shu bilan)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def prf_from_counts(tp: int, fp: int, fn: int):\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1   = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "    return prec, rec, f1\n",
    "\n",
    "def multilabel_micro_macro(Y_true: np.ndarray, Y_pred: np.ndarray) -> dict:\n",
    "    tp = int(((Y_true == 1) & (Y_pred == 1)).sum())\n",
    "    fp = int(((Y_true == 0) & (Y_pred == 1)).sum())\n",
    "    fn = int(((Y_true == 1) & (Y_pred == 0)).sum())\n",
    "    _, _, micro_f1 = prf_from_counts(tp, fp, fn)\n",
    "\n",
    "    f1s = []\n",
    "    for j in range(Y_true.shape[1]):\n",
    "        y = Y_true[:, j]\n",
    "        p = Y_pred[:, j]\n",
    "        tpj = int(((y == 1) & (p == 1)).sum())\n",
    "        fpj = int(((y == 0) & (p == 1)).sum())\n",
    "        fnj = int(((y == 1) & (p == 0)).sum())\n",
    "        _, _, f1j = prf_from_counts(tpj, fpj, fnj)\n",
    "        f1s.append(f1j)\n",
    "\n",
    "    return {\"micro_f1\": float(micro_f1), \"macro_f1\": float(np.mean(f1s))}\n",
    "\n",
    "def score_matrix(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return np.asarray(model.predict_proba(X)), \"proba\"\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return np.asarray(model.decision_function(X)), \"score\"\n",
    "    return np.asarray(model.predict(X)), \"binary\"\n",
    "\n",
    "def tune_thresholds_per_label(Y_true: np.ndarray, scores: np.ndarray, n_thr: int = 61) -> np.ndarray:\n",
    "    n_labels = Y_true.shape[1]\n",
    "    thr_out = np.zeros(n_labels, dtype=np.float32)\n",
    "    q = np.linspace(0.01, 0.99, n_thr)\n",
    "\n",
    "    for j in range(n_labels):\n",
    "        y = Y_true[:, j].astype(np.int8)\n",
    "        s = scores[:, j].astype(np.float32)\n",
    "\n",
    "        if int(y.sum()) == 0:\n",
    "            thr_out[j] = float(np.max(s) + 1.0)\n",
    "            continue\n",
    "\n",
    "        thr_grid = np.unique(np.quantile(s, q))\n",
    "        if thr_grid.size < 10:\n",
    "            mn, mx = float(np.min(s)), float(np.max(s))\n",
    "            thr_grid = np.array([mn], dtype=np.float32) if mn == mx else np.linspace(mn, mx, num=31, dtype=np.float32)\n",
    "\n",
    "        best_f1 = -1.0\n",
    "        best_thr = float(thr_grid[len(thr_grid)//2])\n",
    "\n",
    "        for thr in thr_grid:\n",
    "            pred = (s >= thr).astype(np.int8)\n",
    "            tp = int(((y == 1) & (pred == 1)).sum())\n",
    "            fp = int(((y == 0) & (pred == 1)).sum())\n",
    "            fn = int(((y == 1) & (pred == 0)).sum())\n",
    "            _, _, f1 = prf_from_counts(tp, fp, fn)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thr = float(thr)\n",
    "\n",
    "        thr_out[j] = best_thr\n",
    "\n",
    "    return thr_out\n",
    "\n",
    "def apply_thresholds(scores: np.ndarray, thr: np.ndarray) -> np.ndarray:\n",
    "    return (scores >= thr.reshape(1, -1)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa39abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 — Save helpers\n",
    "def save_best(model, model_tag: str, best_params: dict, thr: np.ndarray, val_metrics: dict, test_metrics: dict):\n",
    "    # save model\n",
    "    model_path = MODEL_SAVE_DIR / f\"{model_tag}.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # thresholds dict\n",
    "    thr_dict = {lab.replace(\"y_\", \"\", 1): float(t) for lab, t in zip(y_cols, thr)}\n",
    "    thr_path = MODEL_SAVE_DIR / f\"{model_tag}_thresholds.json\"\n",
    "    with open(thr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(thr_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # params\n",
    "    params_path = MODEL_SAVE_DIR / f\"{model_tag}_best_params.json\"\n",
    "    with open(params_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # summaries\n",
    "    with open(RESULTS_DIR / f\"{model_tag}_val_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(val_metrics, f, ensure_ascii=False, indent=2)\n",
    "    with open(RESULTS_DIR / f\"{model_tag}_test_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_metrics, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"✅ Saved model:\", model_path.resolve())\n",
    "    print(\"✅ Saved thresholds:\", thr_path.resolve())\n",
    "    print(\"✅ Saved params:\", params_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bu har algoritm uchun 1 trialni real vaqt bilan o‘lchaydi, keyin siz N_TRIALSga ko‘paytirib olamiz.\n",
    "\n",
    "# import time\n",
    "# import optuna\n",
    "\n",
    "# def time_one_trial(objective_fn, name: str):\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     t0 = time.time()\n",
    "#     study.optimize(objective_fn, n_trials=1)\n",
    "#     dt = time.time() - t0\n",
    "#     print(f\"{name}: 1 trial time = {dt:.2f} sec | best={study.best_value:.6f}\")\n",
    "#     return dt\n",
    "\n",
    "# t_logreg = time_one_trial(objective_logreg, \"logreg\")\n",
    "# t_svc    = time_one_trial(objective_linearsvc, \"linearsvc\")\n",
    "# t_sgdll  = time_one_trial(objective_sgd_logloss, \"sgd_logloss\")\n",
    "# t_sgdh   = time_one_trial(objective_sgd_hinge, \"sgd_hinge\")\n",
    "\n",
    "# print(\"\\nRough totals (sec):\")\n",
    "# print(\"logreg total ~\", t_logreg * N_TRIALS)\n",
    "# print(\"linearsvc total ~\", t_svc * N_TRIALS)\n",
    "# print(\"sgd_logloss total ~\", t_sgdll * N_TRIALS)\n",
    "# print(\"sgd_hinge total ~\", t_sgdh * N_TRIALS)\n",
    "# print(\"ALL 4 total ~\", (t_logreg + t_svc + t_sgdll + t_sgdh) * N_TRIALS)\n",
    "\n",
    "#Shundan keyin siz o‘zingizning PCda aniq vaqtni bilasiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fce520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 09:29:40,294]\u001b[0m A new study created in memory with name: logreg_optuna_20260226_005315\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TRAIN_SUBSAMPLE=60000 for trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 09:30:45,000]\u001b[0m Trial 0 finished with value: 0.9625195978972609 and parameters: {'C': 0.9155436618548748, 'class_weight': None}. Best is trial 0 with value: 0.9625195978972609.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:31:55,998]\u001b[0m Trial 1 finished with value: 0.9628884994927603 and parameters: {'C': 1.990722903930924, 'class_weight': None}. Best is trial 1 with value: 0.9628884994927603.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:33:11,691]\u001b[0m Trial 2 finished with value: 0.9587408905760758 and parameters: {'C': 0.3057486557273976, 'class_weight': None}. Best is trial 1 with value: 0.9628884994927603.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:34:49,856]\u001b[0m Trial 3 finished with value: 0.9629062603755487 and parameters: {'C': 2.908676577972516, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:36:21,150]\u001b[0m Trial 4 finished with value: 0.9613153519305376 and parameters: {'C': 4.476009827847025, 'class_weight': None}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:37:55,575]\u001b[0m Trial 5 finished with value: 0.960346563461201 and parameters: {'C': 0.4720535876966126, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:39:23,554]\u001b[0m Trial 6 finished with value: 0.9611593988712672 and parameters: {'C': 1.1170742583233393, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:40:46,034]\u001b[0m Trial 7 finished with value: 0.9600509869484496 and parameters: {'C': 0.4054144182775009, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:41:53,945]\u001b[0m Trial 8 finished with value: 0.9611881846563094 and parameters: {'C': 1.2144894192131561, 'class_weight': None}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:43:04,355]\u001b[0m Trial 9 finished with value: 0.9628331642534353 and parameters: {'C': 1.4857302385890354, 'class_weight': None}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:47:47,461]\u001b[0m Trial 10 finished with value: 0.9613235171859557 and parameters: {'C': 7.158662921618335, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:50:07,570]\u001b[0m Trial 11 finished with value: 0.9629062603755487 and parameters: {'C': 2.7906264440773576, 'class_weight': 'balanced'}. Best is trial 3 with value: 0.9629062603755487.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:52:19,397]\u001b[0m Trial 12 finished with value: 0.9629151410626504 and parameters: {'C': 2.9811067985213984, 'class_weight': 'balanced'}. Best is trial 12 with value: 0.9629151410626504.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:54:15,369]\u001b[0m Trial 13 finished with value: 0.9629151410626504 and parameters: {'C': 3.5265422204655694, 'class_weight': 'balanced'}. Best is trial 12 with value: 0.9629151410626504.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 09:56:03,046]\u001b[0m Trial 14 finished with value: 0.9612961628701804 and parameters: {'C': 5.299484384298936, 'class_weight': 'balanced'}. Best is trial 12 with value: 0.9629151410626504.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST logreg: 0.9629151410626504 {'C': 2.9811067985213984, 'class_weight': 'balanced'}\n",
      "✅ Saved model: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_logreg_best.joblib\n",
      "✅ Saved thresholds: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_logreg_best_thresholds.json\n",
      "✅ Saved params: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_logreg_best_best_params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'micro_f1': 0.9629411005146556, 'macro_f1': 0.9309690257979467},\n",
       " {'micro_f1': 0.9574156205367211, 'macro_f1': 0.916767073048451})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # CELL 4 — Optuna tuning: LogReg (bosib yuborasiz)\n",
    "# def objective_logreg(trial: optuna.Trial) -> float:\n",
    "#     C = trial.suggest_float(\"C\", 0.25, 8.0, log=True)\n",
    "#     cw = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "#     base = LogisticRegression(\n",
    "#         solver=\"liblinear\",\n",
    "#         max_iter=4000,\n",
    "#         C=C,\n",
    "#         class_weight=cw,\n",
    "#     )\n",
    "#     clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "#     clf.fit(X_train, Y_train)\n",
    "\n",
    "#     S_val, _ = score_matrix(clf, X_val)\n",
    "#     thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "#     Y_val_pred = apply_thresholds(S_val, thr)\n",
    "#     m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "#     # trial log\n",
    "#     trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "#     return m[\"micro_f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"logreg_{RUN_ID}\")\n",
    "# study.optimize(objective_logreg, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "# print(\"BEST logreg:\", study.best_value, study.best_params)\n",
    "\n",
    "# # Train best again + save\n",
    "# best_params = study.best_params\n",
    "# base = LogisticRegression(solver=\"liblinear\", max_iter=5000, C=best_params[\"C\"], class_weight=best_params[\"class_weight\"])\n",
    "# best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "# best_model.fit(X_train, Y_train)\n",
    "\n",
    "# S_val, _ = score_matrix(best_model, X_val)\n",
    "# thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "# Y_val_pred = apply_thresholds(S_val, thr)\n",
    "# val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "# S_test, _ = score_matrix(best_model, X_test)\n",
    "# Y_test_pred = apply_thresholds(S_test, thr)\n",
    "# test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "# # save trials\n",
    "# trials_df = study.trials_dataframe()\n",
    "# trials_df.to_csv(RESULTS_DIR / \"logreg_trials.csv\", index=False)\n",
    "\n",
    "# save_best(best_model, \"optuna_logreg_best\", best_params, thr, val_m, test_m)\n",
    "# val_m, test_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 4 — Optuna tuning: LogReg (SUBSAMPLE + PRUNER)\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# -------- SPEED KNOBS (shu yerlarini xohlasangiz o'zgartirasiz) --------\n",
    "TRAIN_SUBSAMPLE = 60000   # 30k/60k/100k; None => full train (sekin)\n",
    "N_THR = 31           # trial ichida 21/31; finalda N_THR ishlatamiz\n",
    "SEED = RANDOM_STATE\n",
    "\n",
    "# -------- 1) Trial uchun train subsample tayyorlash --------\n",
    "if TRAIN_SUBSAMPLE is not None and X_train.shape[0] > TRAIN_SUBSAMPLE:\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    idx = rng.choice(X_train.shape[0], size=TRAIN_SUBSAMPLE, replace=False)\n",
    "    X_tr = X_train[idx]\n",
    "    Y_tr = Y_train[idx]\n",
    "    print(f\"Using TRAIN_SUBSAMPLE={TRAIN_SUBSAMPLE} for trials\")\n",
    "else:\n",
    "    X_tr = X_train\n",
    "    Y_tr = Y_train\n",
    "    print(\"Using FULL train for trials\")\n",
    "\n",
    "def objective_logreg(trial: optuna.Trial) -> float:\n",
    "    C = trial.suggest_float(\"C\", 0.25, 8.0, log=True)\n",
    "    cw = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "    # trialda iter kamroq (tezroq)\n",
    "    base = LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=2500,     # 4000 -> 2500 (tezroq)\n",
    "        C=C,\n",
    "        class_weight=cw,\n",
    "    )\n",
    "    clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "\n",
    "    # !!! o'zgargan joy: subsample fit\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    S_val, _ = score_matrix(clf, X_val)\n",
    "\n",
    "    # !!! o'zgargan joy: trial’da tez threshold\n",
    "    thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "\n",
    "    Y_val_pred = apply_thresholds(S_val, thr)\n",
    "    m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "    trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "    return m[\"micro_f1\"]\n",
    "\n",
    "# -------- 2) Sampler + Pruner qo'shish --------\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner  = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"logreg_{RUN_ID}\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "study.optimize(objective_logreg, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "print(\"BEST logreg:\", study.best_value, study.best_params)\n",
    "\n",
    "# -------- 3) Final: FULL TRAIN + aniq threshold (N_THR) --------\n",
    "best_params = study.best_params\n",
    "base = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=5000,  # finalda kattaroq\n",
    "    C=best_params[\"C\"],\n",
    "    class_weight=best_params[\"class_weight\"],\n",
    ")\n",
    "best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "S_val, _ = score_matrix(best_model, X_val)\n",
    "\n",
    "# finalda aniq threshold\n",
    "thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "\n",
    "Y_val_pred = apply_thresholds(S_val, thr)\n",
    "val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "S_test, _ = score_matrix(best_model, X_test)\n",
    "Y_test_pred = apply_thresholds(S_test, thr)\n",
    "test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "# save trials\n",
    "study.trials_dataframe().to_csv(RESULTS_DIR / \"logreg_trials.csv\", index=False)\n",
    "\n",
    "save_best(best_model, \"optuna_logreg_best\", best_params, thr, val_m, test_m)\n",
    "val_m, test_m\n",
    "\n",
    "# Tezlikni yana oshirish uchun (2 ta knob)\n",
    "\n",
    "# TRAIN_SUBSAMPLE = 30000\n",
    "\n",
    "# N_THR_FAST = 21\n",
    "\n",
    "# 31 minut ketdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 10:04:04,149]\u001b[0m A new study created in memory with name: linearsvc_optuna_20260226_005315\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TRAIN_SUBSAMPLE=60000 for trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 10:04:43,984]\u001b[0m Trial 0 finished with value: 0.9612748267898383 and parameters: {'C': 0.9155436618548748, 'class_weight': None}. Best is trial 0 with value: 0.9612748267898383.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:05:22,900]\u001b[0m Trial 1 finished with value: 0.9612844223148055 and parameters: {'C': 1.990722903930924, 'class_weight': None}. Best is trial 1 with value: 0.9612844223148055.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:05:59,794]\u001b[0m Trial 2 finished with value: 0.9629042076331371 and parameters: {'C': 0.3057486557273976, 'class_weight': None}. Best is trial 2 with value: 0.9629042076331371.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:07:41,625]\u001b[0m Trial 3 finished with value: 0.9628843918906455 and parameters: {'C': 2.908676577972516, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9629042076331371.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:08:24,182]\u001b[0m Trial 4 finished with value: 0.9612844223148055 and parameters: {'C': 4.476009827847025, 'class_weight': None}. Best is trial 2 with value: 0.9629042076331371.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:09:58,815]\u001b[0m Trial 5 finished with value: 0.9613028978947076 and parameters: {'C': 0.4720535876966126, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9629042076331371.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:11:15,902]\u001b[0m Trial 6 finished with value: 0.9629028390119722 and parameters: {'C': 1.1170742583233393, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9629042076331371.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:12:06,582]\u001b[0m Trial 7 finished with value: 0.9629219701162147 and parameters: {'C': 0.4054144182775009, 'class_weight': 'balanced'}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:12:43,705]\u001b[0m Trial 8 finished with value: 0.9612748267898383 and parameters: {'C': 1.2144894192131561, 'class_weight': None}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:13:20,436]\u001b[0m Trial 9 finished with value: 0.9612940176624912 and parameters: {'C': 1.4857302385890354, 'class_weight': None}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:14:14,751]\u001b[0m Trial 10 finished with value: 0.9629028390119722 and parameters: {'C': 0.5133782954113716, 'class_weight': 'balanced'}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:15:16,117]\u001b[0m Trial 11 finished with value: 0.9613043277446769 and parameters: {'C': 0.25662552814698836, 'class_weight': 'balanced'}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:16:28,963]\u001b[0m Trial 12 finished with value: 0.9612947327581429 and parameters: {'C': 0.2581233514513384, 'class_weight': 'balanced'}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:17:07,039]\u001b[0m Trial 13 finished with value: 0.9613323972805202 and parameters: {'C': 0.5183661943937903, 'class_weight': None}. Best is trial 7 with value: 0.9629219701162147.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:17:41,913]\u001b[0m Trial 14 finished with value: 0.9629322191785876 and parameters: {'C': 0.383885657767193, 'class_weight': None}. Best is trial 14 with value: 0.9629322191785876.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST linearsvc: 0.9629322191785876 {'C': 0.383885657767193, 'class_weight': None}\n",
      "✅ Saved model: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_linearsvc_best.joblib\n",
      "✅ Saved thresholds: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_linearsvc_best_thresholds.json\n",
      "✅ Saved params: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_linearsvc_best_best_params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'micro_f1': 0.9612652310875851, 'macro_f1': 0.9089976056718859},\n",
       " {'micro_f1': 0.957016126000827, 'macro_f1': 0.9123174799904918})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # CELL 5 — Optuna tuning: LinearSVC\n",
    "# def objective_linearsvc(trial: optuna.Trial) -> float:\n",
    "#     C = trial.suggest_float(\"C\", 0.25, 8.0, log=True)\n",
    "#     cw = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "#     base = LinearSVC(C=C, class_weight=cw, random_state=RANDOM_STATE)\n",
    "#     clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "#     clf.fit(X_train, Y_train)\n",
    "\n",
    "#     S_val, _ = score_matrix(clf, X_val)\n",
    "#     thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "#     Y_val_pred = apply_thresholds(S_val, thr)\n",
    "#     m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "#     trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "#     return m[\"micro_f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"linearsvc_{RUN_ID}\")\n",
    "# study.optimize(objective_linearsvc, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "# print(\"BEST linearsvc:\", study.best_value, study.best_params)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# base = LinearSVC(C=best_params[\"C\"], class_weight=best_params[\"class_weight\"], random_state=RANDOM_STATE)\n",
    "# best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "# best_model.fit(X_train, Y_train)\n",
    "\n",
    "# S_val, _ = score_matrix(best_model, X_val)\n",
    "# thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "# Y_val_pred = apply_thresholds(S_val, thr)\n",
    "# val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "# S_test, _ = score_matrix(best_model, X_test)\n",
    "# Y_test_pred = apply_thresholds(S_test, thr)\n",
    "# test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "# (pd.DataFrame(study.trials_dataframe())).to_csv(RESULTS_DIR / \"linearsvc_trials.csv\", index=False)\n",
    "# save_best(best_model, \"optuna_linearsvc_best\", best_params, thr, val_m, test_m)\n",
    "# val_m, test_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 5 — Optuna tuning: LinearSVC (SUBSAMPLE + PRUNER)\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- SPEED KNOBS --------\n",
    "TRAIN_SUBSAMPLE = 60000   # 30k/60k/100k; None => full train\n",
    "N_THR_FAST = 31           # trial ichida 21/31; finalda N_THR\n",
    "SEED = RANDOM_STATE\n",
    "\n",
    "# -------- 1) Trial uchun subsample --------\n",
    "if TRAIN_SUBSAMPLE is not None and X_train.shape[0] > TRAIN_SUBSAMPLE:\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    idx = rng.choice(X_train.shape[0], size=TRAIN_SUBSAMPLE, replace=False)\n",
    "    X_tr = X_train[idx]\n",
    "    Y_tr = Y_train[idx]\n",
    "    print(f\"Using TRAIN_SUBSAMPLE={TRAIN_SUBSAMPLE} for trials\")\n",
    "else:\n",
    "    X_tr = X_train\n",
    "    Y_tr = Y_train\n",
    "    print(\"Using FULL train for trials\")\n",
    "\n",
    "def objective_linearsvc(trial: optuna.Trial) -> float:\n",
    "    C = trial.suggest_float(\"C\", 0.25, 8.0, log=True)\n",
    "    cw = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "    base = LinearSVC(C=C, class_weight=cw, random_state=RANDOM_STATE)\n",
    "    clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "\n",
    "    # !!! subsample fit\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    S_val, _ = score_matrix(clf, X_val)\n",
    "\n",
    "    # !!! trial’da tez threshold\n",
    "    thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR_FAST)\n",
    "\n",
    "    Y_val_pred = apply_thresholds(S_val, thr)\n",
    "    m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "    trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "    return m[\"micro_f1\"]\n",
    "\n",
    "# -------- 2) Sampler + Pruner --------\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner  = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"linearsvc_{RUN_ID}\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "study.optimize(objective_linearsvc, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "print(\"BEST linearsvc:\", study.best_value, study.best_params)\n",
    "\n",
    "# -------- 3) Final: FULL TRAIN + aniq threshold --------\n",
    "best_params = study.best_params\n",
    "base = LinearSVC(\n",
    "    C=best_params[\"C\"],\n",
    "    class_weight=best_params[\"class_weight\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "S_val, _ = score_matrix(best_model, X_val)\n",
    "thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)  # finalda N_THR\n",
    "Y_val_pred = apply_thresholds(S_val, thr)\n",
    "val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "S_test, _ = score_matrix(best_model, X_test)\n",
    "Y_test_pred = apply_thresholds(S_test, thr)\n",
    "test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "pd.DataFrame(study.trials_dataframe()).to_csv(RESULTS_DIR / \"linearsvc_trials.csv\", index=False)\n",
    "save_best(best_model, \"optuna_linearsvc_best\", best_params, thr, val_m, test_m)\n",
    "val_m, test_m\n",
    "\n",
    "# 15 minut ketdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d37b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 10:20:01,035]\u001b[0m A new study created in memory with name: sgd_logloss_optuna_20260226_005315\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TRAIN_SUBSAMPLE=60000 for trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 10:21:19,765]\u001b[0m Trial 0 finished with value: 0.9459118311981914 and parameters: {'alpha': 5.611516415334504e-06, 'penalty': 'l2'}. Best is trial 0 with value: 0.9459118311981914.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:22:28,271]\u001b[0m Trial 1 finished with value: 0.9279632104835167 and parameters: {'alpha': 1.575132049977973e-05, 'penalty': 'l2'}. Best is trial 0 with value: 0.9459118311981914.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:23:43,861]\u001b[0m Trial 2 finished with value: 0.9467317073170732 and parameters: {'alpha': 1.306673923805328e-06, 'penalty': 'l2'}. Best is trial 2 with value: 0.9467317073170732.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:26:02,841]\u001b[0m Trial 3 finished with value: 0.9473213180619425 and parameters: {'alpha': 2.6070247583707675e-05, 'penalty': 'elasticnet', 'l1_ratio': 0.7659541126403374}. Best is trial 3 with value: 0.9473213180619425.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:28:47,842]\u001b[0m Trial 4 finished with value: 0.93512661307589 and parameters: {'alpha': 2.6587543983272713e-06, 'penalty': 'elasticnet', 'l1_ratio': 0.3433937943676302}. Best is trial 3 with value: 0.9473213180619425.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:30:01,098]\u001b[0m Trial 5 finished with value: 0.9459632457304007 and parameters: {'alpha': 1.120760621186056e-05, 'penalty': 'l2'}. Best is trial 3 with value: 0.9473213180619425.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:32:19,262]\u001b[0m Trial 6 finished with value: 0.9464358603287691 and parameters: {'alpha': 1.6738085788752145e-05, 'penalty': 'elasticnet', 'l1_ratio': 0.39308947463495336}. Best is trial 3 with value: 0.9473213180619425.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:33:29,645]\u001b[0m Trial 7 finished with value: 0.9502563111442429 and parameters: {'alpha': 8.168455894760166e-06, 'penalty': 'l2'}. Best is trial 7 with value: 0.9502563111442429.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:34:49,526]\u001b[0m Trial 8 finished with value: 0.9434107004200462 and parameters: {'alpha': 1.0677482709481361e-05, 'penalty': 'l2'}. Best is trial 7 with value: 0.9502563111442429.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:36:18,958]\u001b[0m Trial 9 finished with value: 0.9378644229599334 and parameters: {'alpha': 1.640928673064794e-05, 'penalty': 'l2'}. Best is trial 7 with value: 0.9502563111442429.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:38:33,579]\u001b[0m Trial 10 finished with value: 0.9381154743860366 and parameters: {'alpha': 7.767353816237052e-05, 'penalty': 'elasticnet', 'l1_ratio': 0.10603785545303457}. Best is trial 7 with value: 0.9502563111442429.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:40:47,122]\u001b[0m Trial 11 finished with value: 0.9405552874800491 and parameters: {'alpha': 5.5079156352832935e-05, 'penalty': 'elasticnet', 'l1_ratio': 0.8261318362260142}. Best is trial 7 with value: 0.9502563111442429.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:44:20,469]\u001b[0m Trial 12 finished with value: 0.9312033671321982 and parameters: {'alpha': 3.6190846267142264e-05, 'penalty': 'elasticnet', 'l1_ratio': 0.8575695011121194}. Best is trial 7 with value: 0.9502563111442429.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:47:35,268]\u001b[0m Trial 13 finished with value: 0.9513461072034927 and parameters: {'alpha': 4.552621487933352e-06, 'penalty': 'elasticnet', 'l1_ratio': 0.6577247186720756}. Best is trial 13 with value: 0.9513461072034927.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 10:48:44,295]\u001b[0m Trial 14 finished with value: 0.9011061774557918 and parameters: {'alpha': 5.234761772026523e-06, 'penalty': 'l2'}. Best is trial 13 with value: 0.9513461072034927.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST sgd_logloss: 0.9513461072034927 {'alpha': 4.552621487933352e-06, 'penalty': 'elasticnet', 'l1_ratio': 0.6577247186720756}\n",
      "✅ Saved model: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_sgd_logloss_best.joblib\n",
      "✅ Saved thresholds: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_sgd_logloss_best_thresholds.json\n",
      "✅ Saved params: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_sgd_logloss_best_best_params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'micro_f1': 0.9509630339727475, 'macro_f1': 0.9163810555597952},\n",
       " {'micro_f1': 0.9434148304658417, 'macro_f1': 0.9039855387001187})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #CELL 6 — Optuna tuning: SGD log_loss\n",
    "# def objective_sgd_logloss(trial: optuna.Trial) -> float:\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-6, 1e-4, log=True)\n",
    "#     penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"elasticnet\"])\n",
    "#     l1_ratio = None\n",
    "#     if penalty == \"elasticnet\":\n",
    "#         l1_ratio = trial.suggest_float(\"l1_ratio\", 0.1, 0.9)\n",
    "\n",
    "#     base = SGDClassifier(\n",
    "#         loss=\"log_loss\",\n",
    "#         penalty=penalty,\n",
    "#         alpha=alpha,\n",
    "#         l1_ratio=l1_ratio,\n",
    "#         max_iter=3000,\n",
    "#         tol=1e-3,\n",
    "#         random_state=RANDOM_STATE,\n",
    "#     )\n",
    "#     clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "#     clf.fit(X_train, Y_train)\n",
    "\n",
    "#     S_val, _ = score_matrix(clf, X_val)\n",
    "#     thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "#     Y_val_pred = apply_thresholds(S_val, thr)\n",
    "#     m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "#     trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "#     return m[\"micro_f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"sgd_logloss_{RUN_ID}\")\n",
    "# study.optimize(objective_sgd_logloss, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "# print(\"BEST sgd_logloss:\", study.best_value, study.best_params)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# base = SGDClassifier(\n",
    "#     loss=\"log_loss\",\n",
    "#     penalty=best_params[\"penalty\"],\n",
    "#     alpha=best_params[\"alpha\"],\n",
    "#     l1_ratio=best_params.get(\"l1_ratio\", None),\n",
    "#     max_iter=4000,\n",
    "#     tol=1e-3,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "# best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "# best_model.fit(X_train, Y_train)\n",
    "\n",
    "# S_val, _ = score_matrix(best_model, X_val)\n",
    "# thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "# Y_val_pred = apply_thresholds(S_val, thr)\n",
    "# val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "# S_test, _ = score_matrix(best_model, X_test)\n",
    "# Y_test_pred = apply_thresholds(S_test, thr)\n",
    "# test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "# (pd.DataFrame(study.trials_dataframe())).to_csv(RESULTS_DIR / \"sgd_logloss_trials.csv\", index=False)\n",
    "# save_best(best_model, \"optuna_sgd_logloss_best\", best_params, thr, val_m, test_m)\n",
    "# val_m, test_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 6 — Optuna tuning: SGD log_loss (SUBSAMPLE + PRUNER)\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- SPEED KNOBS --------\n",
    "TRAIN_SUBSAMPLE = 60000   # 30k/60k/100k; None => full train (sekin)\n",
    "N_THR_FAST = 31           # trial ichida 21/31; finalda N_THR ishlatamiz\n",
    "SEED = RANDOM_STATE\n",
    "\n",
    "# -------- 1) Trial uchun subsample --------\n",
    "if TRAIN_SUBSAMPLE is not None and X_train.shape[0] > TRAIN_SUBSAMPLE:\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    idx = rng.choice(X_train.shape[0], size=TRAIN_SUBSAMPLE, replace=False)\n",
    "    X_tr = X_train[idx]\n",
    "    Y_tr = Y_train[idx]\n",
    "    print(f\"Using TRAIN_SUBSAMPLE={TRAIN_SUBSAMPLE} for trials\")\n",
    "else:\n",
    "    X_tr = X_train\n",
    "    Y_tr = Y_train\n",
    "    print(\"Using FULL train for trials\")\n",
    "\n",
    "def objective_sgd_logloss(trial: optuna.Trial) -> float:\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-6, 1e-4, log=True)\n",
    "    penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"elasticnet\"])\n",
    "    l1_ratio = None\n",
    "    if penalty == \"elasticnet\":\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.1, 0.9)\n",
    "\n",
    "    base = SGDClassifier(\n",
    "        loss=\"log_loss\",\n",
    "        penalty=penalty,\n",
    "        alpha=alpha,\n",
    "        l1_ratio=l1_ratio,\n",
    "        max_iter=2000,      # 3000 -> 2000 (tezroq)\n",
    "        tol=1e-3,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "\n",
    "    # !!! subsample fit\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    S_val, _ = score_matrix(clf, X_val)\n",
    "\n",
    "    # !!! trial’da tez threshold\n",
    "    thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR_FAST)\n",
    "\n",
    "    Y_val_pred = apply_thresholds(S_val, thr)\n",
    "    m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "    trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "    return m[\"micro_f1\"]\n",
    "\n",
    "# -------- 2) Sampler + Pruner --------\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner  = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"sgd_logloss_{RUN_ID}\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "study.optimize(objective_sgd_logloss, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "print(\"BEST sgd_logloss:\", study.best_value, study.best_params)\n",
    "\n",
    "# -------- 3) Final: FULL TRAIN + aniq threshold --------\n",
    "best_params = study.best_params\n",
    "base = SGDClassifier(\n",
    "    loss=\"log_loss\",\n",
    "    penalty=best_params[\"penalty\"],\n",
    "    alpha=best_params[\"alpha\"],\n",
    "    l1_ratio=best_params.get(\"l1_ratio\", None),\n",
    "    max_iter=4000,   # finalda kattaroq\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "S_val, _ = score_matrix(best_model, X_val)\n",
    "thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)   # finalda N_THR (aniq)\n",
    "Y_val_pred = apply_thresholds(S_val, thr)\n",
    "val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "S_test, _ = score_matrix(best_model, X_test)\n",
    "Y_test_pred = apply_thresholds(S_test, thr)\n",
    "test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "pd.DataFrame(study.trials_dataframe()).to_csv(RESULTS_DIR / \"sgd_logloss_trials.csv\", index=False)\n",
    "save_best(best_model, \"optuna_sgd_logloss_best\", best_params, thr, val_m, test_m)\n",
    "val_m, test_m\n",
    "\n",
    "# 40 minut ketdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3c920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 11:01:20,888]\u001b[0m A new study created in memory with name: sgd_hinge_optuna_20260226_005315\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TRAIN_SUBSAMPLE=60000 for trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-26 11:02:21,243]\u001b[0m Trial 0 finished with value: 0.940609772035623 and parameters: {'alpha': 5.611516415334504e-06, 'class_weight': None}. Best is trial 0 with value: 0.940609772035623.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:03:17,580]\u001b[0m Trial 1 finished with value: 0.925443563608909 and parameters: {'alpha': 1.575132049977973e-05, 'class_weight': None}. Best is trial 0 with value: 0.940609772035623.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:04:10,604]\u001b[0m Trial 2 finished with value: 0.9439522573373857 and parameters: {'alpha': 1.306673923805328e-06, 'class_weight': None}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:04:59,382]\u001b[0m Trial 3 finished with value: 0.9386591634253768 and parameters: {'alpha': 2.6070247583707675e-05, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:05:47,277]\u001b[0m Trial 4 finished with value: 0.9378877302811456 and parameters: {'alpha': 4.6225890010208326e-05, 'class_weight': None}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:06:36,807]\u001b[0m Trial 5 finished with value: 0.9345337314307695 and parameters: {'alpha': 2.3270677083837795e-06, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:07:24,723]\u001b[0m Trial 6 finished with value: 0.9412076949828858 and parameters: {'alpha': 7.30953983591291e-06, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:08:04,898]\u001b[0m Trial 7 finished with value: 0.9106959005165761 and parameters: {'alpha': 1.9010245319870378e-06, 'class_weight': 'balanced'}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:08:58,929]\u001b[0m Trial 8 finished with value: 0.9273855789910577 and parameters: {'alpha': 8.168455894760166e-06, 'class_weight': None}. Best is trial 2 with value: 0.9439522573373857.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:09:50,714]\u001b[0m Trial 9 finished with value: 0.9469473247223767 and parameters: {'alpha': 1.0677482709481361e-05, 'class_weight': None}. Best is trial 9 with value: 0.9469473247223767.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:10:38,110]\u001b[0m Trial 10 finished with value: 0.9442759984699922 and parameters: {'alpha': 7.767353816237052e-05, 'class_weight': None}. Best is trial 9 with value: 0.9469473247223767.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:11:20,028]\u001b[0m Trial 11 finished with value: 0.9412304529770209 and parameters: {'alpha': 8.691089486124968e-05, 'class_weight': None}. Best is trial 9 with value: 0.9469473247223767.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:12:01,781]\u001b[0m Trial 12 finished with value: 0.9385063840487254 and parameters: {'alpha': 8.339148723482912e-05, 'class_weight': None}. Best is trial 9 with value: 0.9469473247223767.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:12:54,912]\u001b[0m Trial 13 finished with value: 0.9423397130081758 and parameters: {'alpha': 2.818826384247118e-05, 'class_weight': None}. Best is trial 9 with value: 0.9469473247223767.\u001b[0m\n",
      "\u001b[32m[I 2026-02-26 11:13:51,133]\u001b[0m Trial 14 finished with value: 0.9393061699650757 and parameters: {'alpha': 4.672941588975312e-06, 'class_weight': None}. Best is trial 9 with value: 0.9469473247223767.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST sgd_hinge: 0.9469473247223767 {'alpha': 1.0677482709481361e-05, 'class_weight': None}\n",
      "✅ Saved model: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_sgd_hinge_best.joblib\n",
      "✅ Saved thresholds: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_sgd_hinge_best_thresholds.json\n",
      "✅ Saved params: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\improvement_models\\Improvement_Models\\Optuna_Tuned_optuna_20260226_005315\\optuna_sgd_hinge_best_best_params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'micro_f1': 0.9480212803102002, 'macro_f1': 0.8888894025923383},\n",
       " {'micro_f1': 0.9380873513593986, 'macro_f1': 0.8953980073993205})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #CELL 7 — Optuna tuning: SGD hinge\n",
    "# def objective_sgd_hinge(trial: optuna.Trial) -> float:\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-6, 1e-4, log=True)\n",
    "#     cw = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "#     base = SGDClassifier(\n",
    "#         loss=\"hinge\",\n",
    "#         penalty=\"l2\",\n",
    "#         alpha=alpha,\n",
    "#         class_weight=cw,\n",
    "#         max_iter=3000,\n",
    "#         tol=1e-3,\n",
    "#         random_state=RANDOM_STATE,\n",
    "#     )\n",
    "#     clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "#     clf.fit(X_train, Y_train)\n",
    "\n",
    "#     S_val, _ = score_matrix(clf, X_val)\n",
    "#     thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "#     Y_val_pred = apply_thresholds(S_val, thr)\n",
    "#     m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "#     trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "#     return m[\"micro_f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\", study_name=f\"sgd_hinge_{RUN_ID}\")\n",
    "# study.optimize(objective_sgd_hinge, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "# print(\"BEST sgd_hinge:\", study.best_value, study.best_params)\n",
    "\n",
    "# best_params = study.best_params\n",
    "# base = SGDClassifier(\n",
    "#     loss=\"hinge\",\n",
    "#     penalty=\"l2\",\n",
    "#     alpha=best_params[\"alpha\"],\n",
    "#     class_weight=best_params[\"class_weight\"],\n",
    "#     max_iter=4000,\n",
    "#     tol=1e-3,\n",
    "#     random_state=RANDOM_STATE,\n",
    "# )\n",
    "# best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "# best_model.fit(X_train, Y_train)\n",
    "\n",
    "# S_val, _ = score_matrix(best_model, X_val)\n",
    "# thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)\n",
    "# Y_val_pred = apply_thresholds(S_val, thr)\n",
    "# val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "# S_test, _ = score_matrix(best_model, X_test)\n",
    "# Y_test_pred = apply_thresholds(S_test, thr)\n",
    "# test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "# (pd.DataFrame(study.trials_dataframe())).to_csv(RESULTS_DIR / \"sgd_hinge_trials.csv\", index=False)\n",
    "# save_best(best_model, \"optuna_sgd_hinge_best\", best_params, thr, val_m, test_m)\n",
    "# val_m, test_m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 7 — Optuna tuning: SGD hinge (SUBSAMPLE + PRUNER)\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- SPEED KNOBS --------\n",
    "TRAIN_SUBSAMPLE = 60000   # 30k/60k/100k; None => full train\n",
    "N_THR_FAST = 31           # trial ichida 21/31; finalda N_THR\n",
    "SEED = RANDOM_STATE\n",
    "\n",
    "# -------- 1) Trial uchun subsample --------\n",
    "if TRAIN_SUBSAMPLE is not None and X_train.shape[0] > TRAIN_SUBSAMPLE:\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    idx = rng.choice(X_train.shape[0], size=TRAIN_SUBSAMPLE, replace=False)\n",
    "    X_tr = X_train[idx]\n",
    "    Y_tr = Y_train[idx]\n",
    "    print(f\"Using TRAIN_SUBSAMPLE={TRAIN_SUBSAMPLE} for trials\")\n",
    "else:\n",
    "    X_tr = X_train\n",
    "    Y_tr = Y_train\n",
    "    print(\"Using FULL train for trials\")\n",
    "\n",
    "def objective_sgd_hinge(trial: optuna.Trial) -> float:\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-6, 1e-4, log=True)\n",
    "    cw = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "\n",
    "    base = SGDClassifier(\n",
    "        loss=\"hinge\",\n",
    "        penalty=\"l2\",\n",
    "        alpha=alpha,\n",
    "        class_weight=cw,\n",
    "        max_iter=2000,    # 3000 -> 2000 (tezroq)\n",
    "        tol=1e-3,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    clf = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "\n",
    "    # !!! subsample fit\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    S_val, _ = score_matrix(clf, X_val)\n",
    "\n",
    "    # !!! trial’da tez threshold\n",
    "    thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR_FAST)\n",
    "\n",
    "    Y_val_pred = apply_thresholds(S_val, thr)\n",
    "    m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "    trial.set_user_attr(\"macro_f1\", m[\"macro_f1\"])\n",
    "    return m[\"micro_f1\"]\n",
    "\n",
    "# -------- 2) Sampler + Pruner --------\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner  = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=0)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=f\"sgd_hinge_{RUN_ID}\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    ")\n",
    "\n",
    "study.optimize(objective_sgd_hinge, n_trials=N_TRIALS, timeout=TIMEOUT_SEC)\n",
    "\n",
    "print(\"BEST sgd_hinge:\", study.best_value, study.best_params)\n",
    "\n",
    "# -------- 3) Final: FULL TRAIN + aniq threshold --------\n",
    "best_params = study.best_params\n",
    "base = SGDClassifier(\n",
    "    loss=\"hinge\",\n",
    "    penalty=\"l2\",\n",
    "    alpha=best_params[\"alpha\"],\n",
    "    class_weight=best_params[\"class_weight\"],\n",
    "    max_iter=4000,   # finalda kattaroq\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "best_model = OneVsRestClassifier(base, n_jobs=N_JOBS)\n",
    "best_model.fit(X_train, Y_train)\n",
    "\n",
    "S_val, _ = score_matrix(best_model, X_val)\n",
    "thr = tune_thresholds_per_label(Y_val, S_val, n_thr=N_THR)  # finalda N_THR\n",
    "Y_val_pred = apply_thresholds(S_val, thr)\n",
    "val_m = multilabel_micro_macro(Y_val, Y_val_pred)\n",
    "\n",
    "S_test, _ = score_matrix(best_model, X_test)\n",
    "Y_test_pred = apply_thresholds(S_test, thr)\n",
    "test_m = multilabel_micro_macro(Y_test, Y_test_pred)\n",
    "\n",
    "pd.DataFrame(study.trials_dataframe()).to_csv(RESULTS_DIR / \"sgd_hinge_trials.csv\", index=False)\n",
    "save_best(best_model, \"optuna_sgd_hinge_best\", best_params, thr, val_m, test_m)\n",
    "val_m, test_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ef091",
   "metadata": {},
   "source": [
    "# “FINAL TRAIN” (LogReg) — bitta cell (train+val bilan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99dc043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Training LogisticRegression (solver=liblinear, C=2.98109399813007, class_weight=balanced, max_iter=6000)\n",
      "==========================================================================================\n",
      "Train time: 5.69 min\n",
      "Threshold calib time: 0.5 sec\n",
      "Total time: 5.70 min\n",
      "\n",
      "VAL: {'model': 'optuna_logreg_best', 'micro_f1': 0.9848308051341891, 'macro_f1': 0.9737848794746311}\n",
      "TEST: {'model': 'optuna_logreg_best', 'micro_f1': 0.9781993259508907, 'macro_f1': 0.9660216617683088}\n",
      "\n",
      "✅ Saved model dir: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Models\\best_model\\optuna_logreg_best\n",
      "✅ Saved results dir: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\results\\tables\\best_model_results\\optuna_logreg_best\n",
      "✅ Timing saved: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\results\\tables\\best_model_results\\optuna_logreg_best\\optuna_logreg_best_timing.json\n"
     ]
    }
   ],
   "source": [
    "# Training LogisticRegression (...)\n",
    "\n",
    "# Train time: ... min\n",
    "\n",
    "# formatlari qo‘shilgan (fit va threshold calibration ham vaqt bilan chiqadi).\n",
    "# Va siz aytgan path’larda saqlaydi:\n",
    "\n",
    "# ✅ Models/best_model/<MODEL_NAME>/...\n",
    "\n",
    "# ✅ results/tables/best_model_results/<MODEL_NAME>/...\n",
    "\n",
    "#  bu cell ishlashi uchun sizda helper funksiyalar bor bo‘lsin:\n",
    "# score_matrix, tune_thresholds_per_label, apply_thresholds\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "VERSION = \"fe_v1_fs_chi2_v1\"\n",
    "PREFER_FEATURE_SELECTED = True\n",
    "\n",
    "MODEL_NAME = \"optuna_logreg_best\"   # folder nomi ham shu bo'ladi\n",
    "\n",
    "# Optuna LogReg best params\n",
    "C_best = 2.98109399813007\n",
    "cw_best = \"balanced\"\n",
    "\n",
    "# =========================\n",
    "# Helpers: pretty timing + titles\n",
    "# =========================\n",
    "def fmt_minutes(seconds: float) -> str:\n",
    "    mins = seconds / 60.0\n",
    "    if mins < 1:\n",
    "        return f\"{seconds:.1f} sec\"\n",
    "    return f\"{mins:.2f} min\"\n",
    "\n",
    "def model_title(name: str, params: dict) -> str:\n",
    "    inside = \", \".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "    return f\"{name} ({inside})\"\n",
    "\n",
    "# =========================\n",
    "# Helpers: project root + data dir\n",
    "# =========================\n",
    "def find_project_root(start=None):\n",
    "    start = start or Path.cwd()\n",
    "    for p in [start] + list(start.parents):\n",
    "        d = p / \"Data\"\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        for sub in [\"Feature_Selected\", \"Engineered_data\", \"Processed\", \"Raw_data\"]:\n",
    "            x = d / sub\n",
    "            if x.exists() and any(x.iterdir()):\n",
    "                return p\n",
    "    return start\n",
    "\n",
    "def find_data_dir(project_root: Path, version: str, prefer_fs: bool = True):\n",
    "    fs = project_root / \"Data\" / \"Feature_Selected\" / version\n",
    "    eng = project_root / \"Data\" / \"Engineered_data\" / version\n",
    "    if prefer_fs and (fs / \"X_train.npz\").exists(): return fs\n",
    "    if (eng / \"X_train.npz\").exists(): return eng\n",
    "    if (fs / \"X_train.npz\").exists(): return fs\n",
    "    raise FileNotFoundError(\"X_train.npz topilmadi (Feature_Selected/Engineered_data).\")\n",
    "\n",
    "def per_label_metrics_df(y_cols, Y_true, Y_pred, thr):\n",
    "    rows=[]\n",
    "    for j, lab in enumerate(y_cols):\n",
    "        y=Y_true[:,j]; p=Y_pred[:,j]\n",
    "        tp=int(((y==1)&(p==1)).sum())\n",
    "        fp=int(((y==0)&(p==1)).sum())\n",
    "        fn=int(((y==1)&(p==0)).sum())\n",
    "        tn=int(((y==0)&(p==0)).sum())\n",
    "        prec= tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "        rec = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        f1  = (2*prec*rec/(prec+rec)) if (prec+rec)>0 else 0.0\n",
    "        rows.append({\"label\": lab, \"support_pos\": int(y.sum()), \"precision\": prec, \"recall\": rec, \"f1\": f1, \"threshold\": float(thr[j])})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "PROJECT_ROOT = find_project_root()\n",
    "DATA_DIR = find_data_dir(PROJECT_ROOT, VERSION, PREFER_FEATURE_SELECTED)\n",
    "\n",
    "with open(DATA_DIR / \"engineered_meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "y_cols = meta[\"y_cols\"]\n",
    "\n",
    "X_train = sparse.load_npz(DATA_DIR / \"X_train.npz\").tocsr()\n",
    "X_val   = sparse.load_npz(DATA_DIR / \"X_val.npz\").tocsr()\n",
    "X_test  = sparse.load_npz(DATA_DIR / \"X_test.npz\").tocsr()\n",
    "\n",
    "Y_train = np.load(DATA_DIR / \"Y_train.npy\")\n",
    "Y_val   = np.load(DATA_DIR / \"Y_val.npy\")\n",
    "Y_test  = np.load(DATA_DIR / \"Y_test.npy\")\n",
    "\n",
    "# =========================\n",
    "# FINAL TRAIN: train+val -> fit/cal split\n",
    "# =========================\n",
    "X_tv = sparse.vstack([X_train, X_val]).tocsr()\n",
    "Y_tv = np.vstack([Y_train, Y_val]).astype(np.int8)\n",
    "\n",
    "idx_all = np.arange(X_tv.shape[0])\n",
    "idx_fit, idx_cal = train_test_split(idx_all, test_size=0.10, random_state=42)\n",
    "\n",
    "X_fit, Y_fit = X_tv[idx_fit], Y_tv[idx_fit]\n",
    "X_cal, Y_cal = X_tv[idx_cal], Y_tv[idx_cal]\n",
    "\n",
    "base = LogisticRegression(solver=\"liblinear\", max_iter=6000, C=C_best, class_weight=cw_best)\n",
    "final_model = OneVsRestClassifier(base, n_jobs=1)\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"LogisticRegression\"\n",
    "MODEL_PARAMS = {\"solver\": \"liblinear\", \"C\": C_best, \"class_weight\": cw_best, \"max_iter\": 6000}\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Training\", model_title(MODEL_DISPLAY_NAME, MODEL_PARAMS))\n",
    "print(\"=\"*90)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "final_model.fit(X_fit, Y_fit)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Train time: {fmt_minutes(t1 - t0)}\")\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "S_cal, _ = score_matrix(final_model, X_cal)\n",
    "thr_final = tune_thresholds_per_label(Y_cal, S_cal, n_thr=61)\n",
    "t3 = time.perf_counter()\n",
    "print(f\"Threshold calib time: {fmt_minutes(t3 - t2)}\")\n",
    "print(f\"Total time: {fmt_minutes((t1 - t0) + (t3 - t2))}\")\n",
    "\n",
    "# =========================\n",
    "# EVAL: VAL + TEST\n",
    "# =========================\n",
    "S_val, _ = score_matrix(final_model, X_val)\n",
    "Y_val_pred = apply_thresholds(S_val, thr_final)\n",
    "\n",
    "S_test, _ = score_matrix(final_model, X_test)\n",
    "Y_test_pred = apply_thresholds(S_test, thr_final)\n",
    "\n",
    "val_micro = f1_score(Y_val, Y_val_pred, average=\"micro\", zero_division=0)\n",
    "val_macro = f1_score(Y_val, Y_val_pred, average=\"macro\", zero_division=0)\n",
    "test_micro = f1_score(Y_test, Y_test_pred, average=\"micro\", zero_division=0)\n",
    "test_macro = f1_score(Y_test, Y_test_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "val_summary = {\"model\": MODEL_NAME, \"micro_f1\": float(val_micro), \"macro_f1\": float(val_macro)}\n",
    "test_summary = {\"model\": MODEL_NAME, \"micro_f1\": float(test_micro), \"macro_f1\": float(test_macro)}\n",
    "\n",
    "val_per_label = per_label_metrics_df(y_cols, Y_val, Y_val_pred, thr_final)\n",
    "test_per_label = per_label_metrics_df(y_cols, Y_test, Y_test_pred, thr_final)\n",
    "\n",
    "timing = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"display\": model_title(MODEL_DISPLAY_NAME, MODEL_PARAMS),\n",
    "    \"train_fit_seconds\": float(t1 - t0),\n",
    "    \"threshold_calib_seconds\": float(t3 - t2),\n",
    "    \"total_seconds\": float((t1 - t0) + (t3 - t2)),\n",
    "    \"train_fit_human\": fmt_minutes(t1 - t0),\n",
    "    \"threshold_calib_human\": fmt_minutes(t3 - t2),\n",
    "    \"total_human\": fmt_minutes((t1 - t0) + (t3 - t2)),\n",
    "    \"X_fit_shape\": [int(X_fit.shape[0]), int(X_fit.shape[1])],\n",
    "    \"X_cal_shape\": [int(X_cal.shape[0]), int(X_cal.shape[1])],\n",
    "}\n",
    "\n",
    "print(\"\\nVAL:\", val_summary)\n",
    "print(\"TEST:\", test_summary)\n",
    "\n",
    "# =========================\n",
    "# SAVE (siz aytgandek)\n",
    "# =========================\n",
    "MODEL_DIR = PROJECT_ROOT / \"Models\" / \"best_model\" / MODEL_NAME\n",
    "RES_DIR = PROJECT_ROOT / \"results\" / \"tables\" / \"best_model_results\" / MODEL_NAME\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# model + thresholds + params\n",
    "joblib.dump(final_model, MODEL_DIR / f\"{MODEL_NAME}.joblib\")\n",
    "\n",
    "thr_dict = {c.replace(\"y_\", \"\", 1): float(t) for c, t in zip(y_cols, thr_final)}\n",
    "with open(MODEL_DIR / f\"{MODEL_NAME}_thresholds.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(thr_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(MODEL_DIR / f\"{MODEL_NAME}_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"C\": C_best, \"class_weight\": cw_best}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# results\n",
    "val_per_label.to_csv(RES_DIR / f\"{MODEL_NAME}_val_per_label_metrics.csv\", index=False)\n",
    "test_per_label.to_csv(RES_DIR / f\"{MODEL_NAME}_test_per_label_metrics.csv\", index=False)\n",
    "\n",
    "with open(RES_DIR / f\"{MODEL_NAME}_val_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_summary, f, ensure_ascii=False, indent=2)\n",
    "with open(RES_DIR / f\"{MODEL_NAME}_test_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(RES_DIR / f\"{MODEL_NAME}_timing.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(timing, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n✅ Saved model dir:\", MODEL_DIR.resolve())\n",
    "print(\"✅ Saved results dir:\", RES_DIR.resolve())\n",
    "print(\"✅ Timing saved:\", (RES_DIR / f\"{MODEL_NAME}_timing.json\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc22c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d07d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b31328c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b4a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_folder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
