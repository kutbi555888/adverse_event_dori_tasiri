{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08396d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 09_feature_engineering.py (notebook cell’lari sifatida ham ishlaydi)\n",
    "# FAERS25Q4 multilabel — feature engineering + featurizer artifact\n",
    "# =========================\n",
    "\n",
    "# =========================================================\n",
    "# CELL 1 — Imports\n",
    "# =========================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f64aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n",
      "SPLIT_DIR: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Processed\\splits_multilabel_noleakage\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CELL 2 — Config + PROJECT_ROOT + SPLIT_DIR (rasmga mos fix)\n",
    "# =========================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Sizda bor split papka (rasmda shu)\n",
    "SPLIT_DIR_CANDIDATES = [\n",
    "    \"splits_multilabel_noleakage\",   # <-- asosiy\n",
    "    \"splits_multilabel_noleakage2\",\n",
    "    \"splits_multilabel_noleakage3\",\n",
    "]\n",
    "\n",
    "TEXT_COL = \"REAC_pt_symptom_v2\"\n",
    "EXCLUDE_Y_COLS = {\"y_labels\", \"y_sum\", \"label_sum\"}\n",
    "\n",
    "FE_VERSION = \"fe_v1\"\n",
    "SAVE_MATRICES_NPZ = False\n",
    "RUN_QUICK_CHECK = True\n",
    "MAX_TRAIN_SAMPLES_FOR_QUICK = 50000\n",
    "\n",
    "\n",
    "def find_project_root_and_split(start: Path | None = None):\n",
    "    \"\"\"\n",
    "    Notebooks 'tuzog'i'ga tushmaslik uchun:\n",
    "    PROJECT_ROOT ni Data borligi bilan emas,\n",
    "    aynan Data/Processed/<split>/train|val|test.csv borligi bilan topadi.\n",
    "    \"\"\"\n",
    "    start = start or Path.cwd()\n",
    "    checked = []\n",
    "\n",
    "    for p in [start] + list(start.parents):\n",
    "        processed = p / \"Data\" / \"Processed\"\n",
    "        if not processed.exists():\n",
    "            continue\n",
    "\n",
    "        for name in SPLIT_DIR_CANDIDATES:\n",
    "            d = processed / name\n",
    "            checked.append(d)\n",
    "            if (d / \"train.csv\").exists() and (d / \"val.csv\").exists() and (d / \"test.csv\").exists():\n",
    "                return p, d\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Split papka topilmadi (parents bo‘ylab qidirildi).\\n\"\n",
    "        f\"CWD: {start.resolve()}\\n\"\n",
    "        \"Tekshirilgan processed/split papkalar (oxirgi 10 ta):\\n\"\n",
    "        + \"\\n\".join(str(x) for x in checked[-10:])\n",
    "    )\n",
    "\n",
    "\n",
    "PROJECT_ROOT, SPLIT_DIR = find_project_root_and_split()\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT.resolve())\n",
    "print(\"SPLIT_DIR:\", SPLIT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "320adbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (201176, 24) | val: (24410, 24) | test: (24164, 24)\n",
      "Num labels: 21\n",
      "First labels: ['y_cardiovascular', 'y_dermatologic', 'y_edema_swelling', 'y_gastrointestinal', 'y_general_systemic', 'y_hematologic', 'y_hepatic', 'y_hypersensitivity_allergy', 'y_infections', 'y_injection_site']\n",
      "Empty text rows (train/val/test): 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CELL 3 — Load split CSVs\n",
    "# =========================================================\n",
    "train_path = SPLIT_DIR / \"train.csv\"\n",
    "val_path = SPLIT_DIR / \"val.csv\"\n",
    "test_path = SPLIT_DIR / \"test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path, low_memory=False)\n",
    "val_df = pd.read_csv(val_path, low_memory=False)\n",
    "test_df = pd.read_csv(test_path, low_memory=False)\n",
    "\n",
    "print(\"train:\", train_df.shape, \"| val:\", val_df.shape, \"| test:\", test_df.shape)\n",
    "\n",
    "if TEXT_COL not in train_df.columns:\n",
    "    raise ValueError(f\"TEXT_COL topilmadi: {TEXT_COL}. Mavjud: {list(train_df.columns)[:30]} ...\")\n",
    "\n",
    "# y_cols tanlash (y_sum/label_sum kirmasin!)\n",
    "y_cols = sorted([c for c in train_df.columns if c.startswith(\"y_\") and c not in EXCLUDE_Y_COLS])\n",
    "if not y_cols:\n",
    "    raise ValueError(\"y_* ustunlari topilmadi (train.csv).\")\n",
    "\n",
    "# val/test ham xuddi shu y_cols ga ega bo‘lsin\n",
    "missing_val = [c for c in y_cols if c not in val_df.columns]\n",
    "missing_test = [c for c in y_cols if c not in test_df.columns]\n",
    "if missing_val or missing_test:\n",
    "    raise ValueError(f\"y_cols mismatch. missing_val={missing_val[:10]} missing_test={missing_test[:10]}\")\n",
    "\n",
    "print(\"Num labels:\", len(y_cols))\n",
    "print(\"First labels:\", y_cols[:10])\n",
    "\n",
    "# basic cleanup\n",
    "for d in (train_df, val_df, test_df):\n",
    "    d[TEXT_COL] = d[TEXT_COL].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "print(\"Empty text rows (train/val/test):\",\n",
    "      int((train_df[TEXT_COL] == \"\").sum()),\n",
    "      int((val_df[TEXT_COL] == \"\").sum()),\n",
    "      int((test_df[TEXT_COL] == \"\").sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722cec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CELL 4 — Meta features (length, term counts)\n",
    "# =========================================================\n",
    "_term_split = re.compile(r\"\\s*;\\s*\")\n",
    "\n",
    "def meta_features(texts: list[str]) -> np.ndarray:\n",
    "    lens = []\n",
    "    n_terms = []\n",
    "    n_uniq_terms = []\n",
    "\n",
    "    for s in texts:\n",
    "        s = (s or \"\").strip()\n",
    "        lens.append(len(s))\n",
    "\n",
    "        if not s:\n",
    "            n_terms.append(0)\n",
    "            n_uniq_terms.append(0)\n",
    "            continue\n",
    "\n",
    "        terms = [t.strip().lower() for t in _term_split.split(s) if t.strip()]\n",
    "        n_terms.append(len(terms))\n",
    "        n_uniq_terms.append(len(set(terms)))\n",
    "\n",
    "    lens = np.array(lens, dtype=np.float32).reshape(-1, 1)\n",
    "    n_terms = np.array(n_terms, dtype=np.float32).reshape(-1, 1)\n",
    "    n_uniq_terms = np.array(n_uniq_terms, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    feats = np.hstack([np.log1p(lens), n_terms, n_uniq_terms]).astype(np.float32)\n",
    "    return feats\n",
    "\n",
    "def meta_to_sparse(texts):\n",
    "    feats = meta_features(list(texts))\n",
    "    return sparse.csr_matrix(feats)\n",
    "\n",
    "meta_transformer = FunctionTransformer(meta_to_sparse, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4212f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureUnion(n_jobs=1,\n",
      "             transformer_list=[('word_tfidf',\n",
      "                                TfidfVectorizer(max_df=0.95, min_df=2,\n",
      "                                                ngram_range=(1, 2),\n",
      "                                                sublinear_tf=True)),\n",
      "                               ('char_tfidf',\n",
      "                                TfidfVectorizer(analyzer='char_wb', max_df=0.98,\n",
      "                                                min_df=3, ngram_range=(3, 5),\n",
      "                                                sublinear_tf=True)),\n",
      "                               ('meta',\n",
      "                                FunctionTransformer(func=<function meta_to_sparse at 0x000002844A03D580>))])\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CELL 5 — Featurizer: word TF-IDF + char_wb TF-IDF + meta\n",
    "# =========================================================\n",
    "word_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    norm=\"l2\",\n",
    ")\n",
    "\n",
    "char_tfidf = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=3,\n",
    "    max_df=0.98,\n",
    "    sublinear_tf=True,\n",
    "    norm=\"l2\",\n",
    ")\n",
    "\n",
    "featurizer = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        (\"word_tfidf\", word_tfidf),\n",
    "        (\"char_tfidf\", char_tfidf),\n",
    "        (\"meta\", meta_transformer),\n",
    "    ],\n",
    "    n_jobs=1,  # Windows/OneDrive uchun 1 tavsiya\n",
    ")\n",
    "\n",
    "print(featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5c0fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting featurizer on train...\n",
      "X_train: (201176, 104479) nnz: 29288234\n",
      "X_val  : (24410, 104479) nnz: 3331217\n",
      "X_test : (24164, 104479) nnz: 3587450\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CELL 6 — Fit on train, transform val/test\n",
    "# =========================================================\n",
    "X_train_text = train_df[TEXT_COL].tolist()\n",
    "X_val_text = val_df[TEXT_COL].tolist()\n",
    "X_test_text = test_df[TEXT_COL].tolist()\n",
    "\n",
    "print(\"Fitting featurizer on train...\")\n",
    "X_train = featurizer.fit_transform(X_train_text)\n",
    "X_val = featurizer.transform(X_val_text)\n",
    "X_test = featurizer.transform(X_test_text)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"nnz:\", X_train.nnz)\n",
    "print(\"X_val  :\", X_val.shape, \"nnz:\", X_val.nnz)\n",
    "print(\"X_test :\", X_test.shape, \"nnz:\", X_test.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f949cf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n",
      "CWD          = C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_project_root(start: Path | None = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"Data\").exists() and (p / \"Models\").exists():\n",
    "            return p\n",
    "        # fallback: Data bo‘lsa ham root deb olamiz\n",
    "        if (p / \"Data\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "# ✅ hozirgi ish papkani rootga o‘tkazamiz\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT.resolve())\n",
    "print(\"CWD          =\", Path.cwd().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65bcb302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n",
      "CWD = C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# PROJECT_ROOT oldin aniqlangan bo‘lsin\n",
    "%cd {PROJECT_ROOT}\n",
    "print(\"CWD =\", Path.cwd().resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc58576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved featurizer: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Engineered_data\\fe_v1\\tfidf_vectorizer.joblib\n",
      "Saved meta: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Engineered_data\\fe_v1\\meta.json\n",
      "SAVE_MATRICES_NPZ=False (matritsa saqlanmadi).\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CELL 7 — Save artifacts (featurizer + optional matrices)\n",
    "# =========================================================\n",
    "ART_DIR = PROJECT_ROOT /\"Data\"/ \"Engineered_data\" / FE_VERSION\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "featurizer_path = ART_DIR / \"tfidf_vectorizer.joblib\"\n",
    "joblib.dump(featurizer, featurizer_path)\n",
    "print(\"Saved featurizer:\", featurizer_path.resolve())\n",
    "\n",
    "# # (qo‘shimcha) aynan shu featurizer'ni \"tfidf_vectorizer.joblib\" nomi bilan ham saqlab qo‘yamiz\n",
    "# joblib.dump(featurizer, ART_DIR / \"tfidf_vectorizer.joblib\")\n",
    "# print(\"Saved tfidf_vectorizer.joblib:\", (ART_DIR / \"tfidf_vectorizer.joblib\").resolve())\n",
    "\n",
    "meta_path = ART_DIR / \"meta.json\"\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"TEXT_COL\": TEXT_COL,\n",
    "            \"y_cols_count\": len(y_cols),\n",
    "            \"y_cols\": y_cols,\n",
    "            \"split_dir\": str(SPLIT_DIR),\n",
    "            \"fe_version\": FE_VERSION,\n",
    "            \"word_tfidf\": {\n",
    "                \"ngram_range\": word_tfidf.ngram_range,\n",
    "                \"min_df\": word_tfidf.min_df,\n",
    "                \"max_df\": word_tfidf.max_df,\n",
    "                \"sublinear_tf\": word_tfidf.sublinear_tf,\n",
    "                \"norm\": word_tfidf.norm,\n",
    "            },\n",
    "            \"char_tfidf\": {\n",
    "                \"analyzer\": char_tfidf.analyzer,\n",
    "                \"ngram_range\": char_tfidf.ngram_range,\n",
    "                \"min_df\": char_tfidf.min_df,\n",
    "                \"max_df\": char_tfidf.max_df,\n",
    "                \"sublinear_tf\": char_tfidf.sublinear_tf,\n",
    "                \"norm\": char_tfidf.norm,\n",
    "            },\n",
    "            \"meta_features\": [\"log1p_len\", \"n_terms\", \"n_uniq_terms\"],\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "print(\"Saved meta:\", meta_path.resolve())\n",
    "\n",
    "if SAVE_MATRICES_NPZ:\n",
    "    sparse.save_npz(ART_DIR / \"X_train.npz\", X_train)\n",
    "    sparse.save_npz(ART_DIR / \"X_val.npz\", X_val)\n",
    "    sparse.save_npz(ART_DIR / \"X_test.npz\", X_test)\n",
    "    print(\"Saved matrices (.npz) into:\", ART_DIR.resolve())\n",
    "else:\n",
    "    print(\"SAVE_MATRICES_NPZ=False (matritsa saqlanmadi).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa982467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quick OVR LogReg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\env_folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL F1 micro=0.9873 | macro=0.9787\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# CELL 8 — Quick sanity-check (optional): OneVsRest(LogReg) on VAL\n",
    "# =========================================================\n",
    "if RUN_QUICK_CHECK:\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    Y_train = train_df[y_cols].astype(int).to_numpy()\n",
    "    Y_val = val_df[y_cols].astype(int).to_numpy()\n",
    "\n",
    "    # subsample (tez bo‘lishi uchun)\n",
    "    if MAX_TRAIN_SAMPLES_FOR_QUICK and len(train_df) > MAX_TRAIN_SAMPLES_FOR_QUICK:\n",
    "        idx = np.random.RandomState(42).choice(len(train_df), size=MAX_TRAIN_SAMPLES_FOR_QUICK, replace=False)\n",
    "        X_tr = X_train[idx]\n",
    "        Y_tr = Y_train[idx]\n",
    "    else:\n",
    "        X_tr = X_train\n",
    "        Y_tr = Y_train\n",
    "\n",
    "    base = LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        max_iter=2000,\n",
    "        n_jobs=1,\n",
    "        class_weight=\"balanced\",\n",
    "    )\n",
    "    clf = OneVsRestClassifier(base, n_jobs=1)\n",
    "\n",
    "    print(\"Training quick OVR LogReg...\")\n",
    "    clf.fit(X_tr, Y_tr)\n",
    "\n",
    "    # proba bo‘lmasa decision_function\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        P = clf.predict_proba(X_val)\n",
    "        Y_pred = (P >= 0.5).astype(int)\n",
    "    else:\n",
    "        S = clf.decision_function(X_val)\n",
    "        Y_pred = (S >= 0).astype(int)\n",
    "\n",
    "    micro = f1_score(Y_val, Y_pred, average=\"micro\", zero_division=0)\n",
    "    macro = f1_score(Y_val, Y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"VAL F1 micro={micro:.4f} | macro={macro:.4f}\")\n",
    "else:\n",
    "    print(\"RUN_QUICK_CHECK=False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d7e2cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGINEERED_DIR: C:\\Users\\xolmu\\OneDrive\\Desktop\\Modul Program oyi\\Modul_Program3\\6_project_dori_tasiri_extract\\Data\\Engineered_data\\fe_v1\n",
      "Saved X_*.npz\n",
      "Saved Y_*.npy\n",
      "Saved engineered_meta.json\n",
      "Saved ids_*.csv: ['primaryid']\n"
     ]
    }
   ],
   "source": [
    "# Bu cell:\n",
    "\n",
    "# Data/Engineered_data/<FE_VERSION>/ papka yaratadi\n",
    "\n",
    "# X_train / X_val / X_test ni .npz qilib saqlaydi\n",
    "\n",
    "# Y_train / Y_val / Y_test ni .npy qilib saqlaydi\n",
    "\n",
    "# y_cols + TEXT_COL meta’ni json qilib saqlaydi\n",
    "\n",
    "# (agar bo‘lsa) primaryid/caseid kabi ID ustunlarni ham saqlab qo‘yadi\n",
    "\n",
    "# =========================================================\n",
    "# CELL 9 — Save Engineered Data into Data/Engineered_data\n",
    "# =========================================================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "ENGINEERED_DIR = PROJECT_ROOT / \"Data\" / \"Engineered_data\" / FE_VERSION\n",
    "ENGINEERED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ENGINEERED_DIR:\", ENGINEERED_DIR.resolve())\n",
    "\n",
    "# --- 1) Save X matrices (sparse) ---\n",
    "sparse.save_npz(ENGINEERED_DIR / \"X_train.npz\", X_train)\n",
    "sparse.save_npz(ENGINEERED_DIR / \"X_val.npz\", X_val)\n",
    "sparse.save_npz(ENGINEERED_DIR / \"X_test.npz\", X_test)\n",
    "print(\"Saved X_*.npz\")\n",
    "\n",
    "# --- 2) Save Y arrays ---\n",
    "Y_train = train_df[y_cols].astype(np.int8).to_numpy()\n",
    "Y_val   = val_df[y_cols].astype(np.int8).to_numpy()\n",
    "Y_test  = test_df[y_cols].astype(np.int8).to_numpy()\n",
    "\n",
    "np.save(ENGINEERED_DIR / \"Y_train.npy\", Y_train)\n",
    "np.save(ENGINEERED_DIR / \"Y_val.npy\", Y_val)\n",
    "np.save(ENGINEERED_DIR / \"Y_test.npy\", Y_test)\n",
    "print(\"Saved Y_*.npy\")\n",
    "\n",
    "# --- 3) Save metadata (y_cols, text_col, shapes) ---\n",
    "meta = {\n",
    "    \"FE_VERSION\": FE_VERSION,\n",
    "    \"TEXT_COL\": TEXT_COL,\n",
    "    \"y_cols\": y_cols,\n",
    "    \"split_dir\": str(SPLIT_DIR),\n",
    "    \"X_shapes\": {\n",
    "        \"train\": [int(X_train.shape[0]), int(X_train.shape[1])],\n",
    "        \"val\":   [int(X_val.shape[0]), int(X_val.shape[1])],\n",
    "        \"test\":  [int(X_test.shape[0]), int(X_test.shape[1])],\n",
    "    },\n",
    "    \"Y_shapes\": {\n",
    "        \"train\": [int(Y_train.shape[0]), int(Y_train.shape[1])],\n",
    "        \"val\":   [int(Y_val.shape[0]), int(Y_val.shape[1])],\n",
    "        \"test\":  [int(Y_test.shape[0]), int(Y_test.shape[1])],\n",
    "    },\n",
    "}\n",
    "with open(ENGINEERED_DIR / \"engineered_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved engineered_meta.json\")\n",
    "\n",
    "# --- 4) (Optional) Save ID columns to align predictions later ---\n",
    "# Sizda qaysi ID borligini bilmasak ham, odatda shu nomlar uchraydi:\n",
    "ID_CANDIDATES = [\"primaryid\", \"caseid\", \"CASEID\", \"PRIMARYID\", \"safetyreportid\"]\n",
    "id_cols = [c for c in ID_CANDIDATES if c in train_df.columns]\n",
    "\n",
    "if id_cols:\n",
    "    train_df[id_cols].to_csv(ENGINEERED_DIR / \"ids_train.csv\", index=False)\n",
    "    val_df[id_cols].to_csv(ENGINEERED_DIR / \"ids_val.csv\", index=False)\n",
    "    test_df[id_cols].to_csv(ENGINEERED_DIR / \"ids_test.csv\", index=False)\n",
    "    print(\"Saved ids_*.csv:\", id_cols)\n",
    "else:\n",
    "    # Hech bo‘lmasa index saqlab qo‘yamiz\n",
    "    np.save(ENGINEERED_DIR / \"idx_train.npy\", train_df.index.to_numpy())\n",
    "    np.save(ENGINEERED_DIR / \"idx_val.npy\", val_df.index.to_numpy())\n",
    "    np.save(ENGINEERED_DIR / \"idx_test.npy\", test_df.index.to_numpy())\n",
    "    print(\"No ID cols found — saved idx_*.npy\")\n",
    "    \n",
    "# Muhim eslatma:\n",
    "\n",
    "# Data/Engineered_data/fe_v1/X_train.npz — treningda ishlatiladigan X\n",
    "\n",
    "# Data/Engineered_data/fe_v1/Y_train.npy — label matriksa\n",
    "\n",
    "# engineered_meta.json — y_cols tartibini saqlab beradi (juda muhim!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c4da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de2437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e8664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_folder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
